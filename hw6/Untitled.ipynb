{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "from keras.layers import Input, Embedding, Flatten, Add, Dot, Concatenate, Dense\n",
    "from keras.layers.core import Dropout\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import pandas as pd\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  1.  1.  1.  1.]\n",
      " [ 1.  1.  1.  1.  1.]]\n"
     ]
    }
   ],
   "source": [
    "a = np.ones([2, 5])\n",
    "b = np.zeros([2, 5])\n",
    "input_a = keras.layers.Input(shape=[5])\n",
    "input_b = keras.layers.Input(shape=[5])\n",
    "out = keras.layers.Add()([input_a, input_b])\n",
    "model = keras.models.Model([input_a,input_b],out)\n",
    "print(model.predict([a,b]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = open('train.csv','r')\n",
    "tf.readline()\n",
    "x_train = []\n",
    "for line in tf:\n",
    "    start = line.find(',')\n",
    "    end = line.find('\\n')\n",
    "    line = line[start:end]\n",
    "    x_train.append(line.split(',')[1:])\n",
    "x_train = np.array(x_train)\n",
    "x_train = x_train.astype(float)\n",
    "x_train_user = x_train[:,0].astype(int)\n",
    "x_train_item = x_train[:,1].astype(int)\n",
    "y_train = x_train[:,2]\n",
    "y_mean = np.mean(y_train)\n",
    "y_std = np.std(y_train)\n",
    "#y_train = (y_train - y_mean) / y_std\n",
    "np.random.seed(1)\n",
    "np.random.shuffle(x_train_user)\n",
    "np.random.seed(1)\n",
    "np.random.shuffle(x_train_item)\n",
    "np.random.seed(1)\n",
    "np.random.shuffle(y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2707\n",
      "1228\n",
      "4.0\n"
     ]
    }
   ],
   "source": [
    "print(x_train_user[0])\n",
    "print(x_train_item[0])\n",
    "print(y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(n_users, n_items, latent_dim = 10):\n",
    "    user_input = Input(shape=[1])\n",
    "    item_input = Input(shape=[1])\n",
    "    user_vec = Embedding(n_users, latent_dim, embeddings_initializer='random_normal')(user_input)\n",
    "    user_vec = Flatten()(user_vec)\n",
    "    item_vec = Embedding(n_items, latent_dim, embeddings_initializer='random_normal')(item_input)\n",
    "    item_vec = Flatten()(item_vec)\n",
    "    user_bias = Embedding(n_users, 1, embeddings_initializer='zeros')(user_input)\n",
    "    user_bias = Flatten()(user_bias)\n",
    "    item_bias = Embedding(n_items, 1, embeddings_initializer='zeros')(item_input)\n",
    "    item_bias = Flatten()(item_bias)\n",
    "    #user_vec  = keras.layers.normalization.BatchNormalization()(user_vec)\n",
    "    #item_vec = keras.layers.normalization.BatchNormalization()(item_vec)\n",
    "    r_hat = Dot(axes=1)([user_vec, item_vec])\n",
    "    r_hat = Add()([r_hat,item_bias,user_bias])#user_bias,item_bias\n",
    "    model = keras.models.Model([user_input, item_input], r_hat)\n",
    "    model.compile(loss='mse', optimizer='sgd')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Input' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-e30594f81971>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mx_train_item\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mitem_tag\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mitem_tag\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train_item\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_tag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_tag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlatent_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-67f9a5a518fa>\u001b[0m in \u001b[0;36mget_model\u001b[0;34m(n_users, n_items, latent_dim)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_users\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_items\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlatent_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0muser_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mitem_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0muser_vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_users\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlatent_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membeddings_initializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'random_normal'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0muser_vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFlatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_vec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Input' is not defined"
     ]
    }
   ],
   "source": [
    "user_tag = []\n",
    "item_tag = []\n",
    "latent_dim = 10\n",
    "for i in range(x_train.shape[0]):\n",
    "    if x_train_user[i] not in user_tag:\n",
    "        user_tag.append(x_train_user[i])\n",
    "    if x_train_item[i] not in item_tag:\n",
    "        item_tag.append(x_train_item[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model(len(user_tag), len(item_tag), latent_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 799873 samples, validate on 100000 samples\n",
      "Epoch 1/1000\n",
      "799873/799873 [==============================] - 15s - loss: 0.9741 - val_loss: 0.9534\n",
      "Epoch 2/1000\n",
      "799873/799873 [==============================] - 14s - loss: 0.9313 - val_loss: 0.9169\n",
      "Epoch 3/1000\n",
      "799873/799873 [==============================] - 14s - loss: 0.8988 - val_loss: 0.8886\n",
      "Epoch 4/1000\n",
      "799873/799873 [==============================] - 15s - loss: 0.8731 - val_loss: 0.8660\n",
      "Epoch 5/1000\n",
      "799873/799873 [==============================] - 15s - loss: 0.8523 - val_loss: 0.8474\n",
      "Epoch 6/1000\n",
      "799873/799873 [==============================] - 15s - loss: 0.8350 - val_loss: 0.8319\n",
      "Epoch 7/1000\n",
      "799873/799873 [==============================] - 15s - loss: 0.8204 - val_loss: 0.8188\n",
      "Epoch 8/1000\n",
      "799873/799873 [==============================] - 15s - loss: 0.8079 - val_loss: 0.8075\n",
      "Epoch 9/1000\n",
      "799873/799873 [==============================] - 15s - loss: 0.7971 - val_loss: 0.7976\n",
      "Epoch 10/1000\n",
      "799873/799873 [==============================] - 15s - loss: 0.7877 - val_loss: 0.7890\n",
      "Epoch 11/1000\n",
      "799873/799873 [==============================] - 16s - loss: 0.7793 - val_loss: 0.7814\n",
      "Epoch 12/1000\n",
      "799873/799873 [==============================] - 16s - loss: 0.7719 - val_loss: 0.7746\n",
      "Epoch 13/1000\n",
      "799873/799873 [==============================] - 16s - loss: 0.7653 - val_loss: 0.7685\n",
      "Epoch 14/1000\n",
      "799873/799873 [==============================] - 16s - loss: 0.7593 - val_loss: 0.7631\n",
      "Epoch 15/1000\n",
      "799873/799873 [==============================] - 15s - loss: 0.7539 - val_loss: 0.7581\n",
      "Epoch 16/1000\n",
      "799873/799873 [==============================] - 15s - loss: 0.7490 - val_loss: 0.7536\n",
      "Epoch 17/1000\n",
      "799873/799873 [==============================] - 16s - loss: 0.7446 - val_loss: 0.7495\n",
      "Epoch 18/1000\n",
      "799873/799873 [==============================] - 16s - loss: 0.7405 - val_loss: 0.7458\n",
      "Epoch 19/1000\n",
      "799873/799873 [==============================] - 15s - loss: 0.7367 - val_loss: 0.7423\n",
      "Epoch 20/1000\n",
      "799873/799873 [==============================] - 15s - loss: 0.7332 - val_loss: 0.7392\n",
      "Epoch 21/1000\n",
      "799873/799873 [==============================] - 16s - loss: 0.7300 - val_loss: 0.7362\n",
      "Epoch 22/1000\n",
      "799873/799873 [==============================] - 16s - loss: 0.7270 - val_loss: 0.7335\n",
      "Epoch 23/1000\n",
      "799873/799873 [==============================] - 16s - loss: 0.7242 - val_loss: 0.7309\n",
      "Epoch 24/1000\n",
      "799873/799873 [==============================] - 16s - loss: 0.7216 - val_loss: 0.7286\n",
      "Epoch 25/1000\n",
      "799873/799873 [==============================] - 16s - loss: 0.7192 - val_loss: 0.7264\n",
      "Epoch 26/1000\n",
      "799873/799873 [==============================] - 16s - loss: 0.7169 - val_loss: 0.7243\n",
      "Epoch 27/1000\n",
      "799873/799873 [==============================] - 15s - loss: 0.7148 - val_loss: 0.7223\n",
      "Epoch 28/1000\n",
      "799873/799873 [==============================] - 16s - loss: 0.7128 - val_loss: 0.7205\n",
      "Epoch 29/1000\n",
      "799873/799873 [==============================] - 17s - loss: 0.7109 - val_loss: 0.7188\n",
      "Epoch 30/1000\n",
      "799873/799873 [==============================] - 16s - loss: 0.7091 - val_loss: 0.7172\n",
      "Epoch 31/1000\n",
      "799873/799873 [==============================] - 16s - loss: 0.7074 - val_loss: 0.7157\n",
      "Epoch 32/1000\n",
      "799873/799873 [==============================] - 16s - loss: 0.7058 - val_loss: 0.7142\n",
      "Epoch 33/1000\n",
      "799873/799873 [==============================] - 16s - loss: 0.7043 - val_loss: 0.7128\n",
      "Epoch 34/1000\n",
      "799873/799873 [==============================] - 16s - loss: 0.7028 - val_loss: 0.7115\n",
      "Epoch 35/1000\n",
      "799873/799873 [==============================] - 15s - loss: 0.7015 - val_loss: 0.7103\n",
      "Epoch 36/1000\n",
      "799873/799873 [==============================] - 16s - loss: 0.7001 - val_loss: 0.7091\n",
      "Epoch 37/1000\n",
      "799873/799873 [==============================] - 16s - loss: 0.6989 - val_loss: 0.7080\n",
      "Epoch 38/1000\n",
      "799873/799873 [==============================] - 15s - loss: 0.6977 - val_loss: 0.7069\n",
      "Epoch 39/1000\n",
      "799873/799873 [==============================] - 14s - loss: 0.6966 - val_loss: 0.7059\n",
      "Epoch 40/1000\n",
      "799873/799873 [==============================] - 13s - loss: 0.6955 - val_loss: 0.7049\n",
      "Epoch 41/1000\n",
      "799873/799873 [==============================] - 13s - loss: 0.6944 - val_loss: 0.7040\n",
      "Epoch 42/1000\n",
      "799873/799873 [==============================] - 13s - loss: 0.6934 - val_loss: 0.7031\n",
      "Epoch 43/1000\n",
      "799873/799873 [==============================] - 13s - loss: 0.6924 - val_loss: 0.7023\n",
      "Epoch 44/1000\n",
      "799873/799873 [==============================] - 14s - loss: 0.6915 - val_loss: 0.7014\n",
      "Epoch 45/1000\n",
      "799873/799873 [==============================] - 13s - loss: 0.6906 - val_loss: 0.7007\n",
      "Epoch 46/1000\n",
      "799873/799873 [==============================] - 13s - loss: 0.6898 - val_loss: 0.6999\n",
      "Epoch 47/1000\n",
      "799873/799873 [==============================] - 13s - loss: 0.6889 - val_loss: 0.6992\n",
      "Epoch 48/1000\n",
      "799873/799873 [==============================] - 13s - loss: 0.6881 - val_loss: 0.6985\n",
      "Epoch 49/1000\n",
      "799873/799873 [==============================] - 13s - loss: 0.6874 - val_loss: 0.6978\n",
      "Epoch 50/1000\n",
      "799873/799873 [==============================] - 13s - loss: 0.6866 - val_loss: 0.6972\n",
      "Epoch 51/1000\n",
      "799873/799873 [==============================] - 13s - loss: 0.6859 - val_loss: 0.6966\n",
      "Epoch 52/1000\n",
      "799873/799873 [==============================] - 14s - loss: 0.6852 - val_loss: 0.6960\n",
      "Epoch 53/1000\n",
      "799873/799873 [==============================] - 13s - loss: 0.6846 - val_loss: 0.6954\n",
      "Epoch 54/1000\n",
      "799873/799873 [==============================] - 13s - loss: 0.6839 - val_loss: 0.6948\n",
      "Epoch 55/1000\n",
      "799873/799873 [==============================] - 13s - loss: 0.6833 - val_loss: 0.6943\n",
      "Epoch 56/1000\n",
      "799873/799873 [==============================] - 13s - loss: 0.6827 - val_loss: 0.6938\n",
      "Epoch 57/1000\n",
      "799873/799873 [==============================] - 13s - loss: 0.6821 - val_loss: 0.6933\n",
      "Epoch 58/1000\n",
      "799873/799873 [==============================] - 13s - loss: 0.6816 - val_loss: 0.6928\n",
      "Epoch 59/1000\n",
      "799873/799873 [==============================] - 14s - loss: 0.6810 - val_loss: 0.6923\n",
      "Epoch 60/1000\n",
      "799873/799873 [==============================] - 16s - loss: 0.6805 - val_loss: 0.6919\n",
      "Epoch 61/1000\n",
      "799873/799873 [==============================] - 16s - loss: 0.6800 - val_loss: 0.6914\n",
      "Epoch 62/1000\n",
      "799873/799873 [==============================] - 17s - loss: 0.6795 - val_loss: 0.6910\n",
      "Epoch 63/1000\n",
      "799873/799873 [==============================] - 16s - loss: 0.6790 - val_loss: 0.6906\n",
      "Epoch 64/1000\n",
      "799873/799873 [==============================] - 14s - loss: 0.6785 - val_loss: 0.6902\n",
      "Epoch 65/1000\n",
      "799873/799873 [==============================] - 13s - loss: 0.6780 - val_loss: 0.6898\n",
      "Epoch 66/1000\n",
      "799873/799873 [==============================] - 13s - loss: 0.6776 - val_loss: 0.6895\n",
      "Epoch 67/1000\n",
      "799873/799873 [==============================] - 13s - loss: 0.6772 - val_loss: 0.6891\n",
      "Epoch 68/1000\n",
      "799873/799873 [==============================] - 14s - loss: 0.6767 - val_loss: 0.6887\n",
      "Epoch 69/1000\n",
      "799873/799873 [==============================] - 14s - loss: 0.6763 - val_loss: 0.6884\n",
      "Epoch 70/1000\n",
      "799873/799873 [==============================] - 13s - loss: 0.6759 - val_loss: 0.6881\n",
      "Epoch 71/1000\n",
      "799873/799873 [==============================] - 13s - loss: 0.6755 - val_loss: 0.6877\n",
      "Epoch 72/1000\n",
      "799873/799873 [==============================] - 13s - loss: 0.6752 - val_loss: 0.6874\n",
      "Epoch 73/1000\n",
      "799873/799873 [==============================] - 13s - loss: 0.6748 - val_loss: 0.6871\n",
      "Epoch 74/1000\n",
      "799873/799873 [==============================] - 14s - loss: 0.6744 - val_loss: 0.6868\n",
      "Epoch 75/1000\n",
      "799873/799873 [==============================] - 13s - loss: 0.6741 - val_loss: 0.6865\n",
      "Epoch 76/1000\n",
      "799873/799873 [==============================] - 13s - loss: 0.6737 - val_loss: 0.6863\n",
      "Epoch 77/1000\n",
      "799873/799873 [==============================] - 13s - loss: 0.6734 - val_loss: 0.6860\n",
      "Epoch 78/1000\n",
      "799873/799873 [==============================] - 13s - loss: 0.6731 - val_loss: 0.6857\n",
      "Epoch 79/1000\n",
      "799873/799873 [==============================] - 13s - loss: 0.6728 - val_loss: 0.6855\n",
      "Epoch 80/1000\n",
      "799873/799873 [==============================] - 13s - loss: 0.6724 - val_loss: 0.6852\n",
      "Epoch 81/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "799873/799873 [==============================] - 13s - loss: 0.6721 - val_loss: 0.6850\n",
      "Epoch 82/1000\n",
      "799873/799873 [==============================] - 13s - loss: 0.6718 - val_loss: 0.6847\n",
      "Epoch 83/1000\n",
      "799873/799873 [==============================] - 13s - loss: 0.6716 - val_loss: 0.6845\n",
      "Epoch 84/1000\n",
      "799873/799873 [==============================] - 13s - loss: 0.6713 - val_loss: 0.6843\n",
      "Epoch 85/1000\n",
      "799873/799873 [==============================] - 13s - loss: 0.6710 - val_loss: 0.6841\n",
      "Epoch 86/1000\n",
      "799873/799873 [==============================] - 14s - loss: 0.6707 - val_loss: 0.6838\n",
      "Epoch 87/1000\n",
      "799873/799873 [==============================] - 16s - loss: 0.6705 - val_loss: 0.6836\n",
      "Epoch 88/1000\n",
      "799873/799873 [==============================] - 14s - loss: 0.6702 - val_loss: 0.6834\n",
      "Epoch 89/1000\n",
      "799873/799873 [==============================] - 13s - loss: 0.6699 - val_loss: 0.6832\n",
      "Epoch 90/1000\n",
      "799873/799873 [==============================] - 13s - loss: 0.6697 - val_loss: 0.6830\n",
      "Epoch 91/1000\n",
      "799873/799873 [==============================] - 13s - loss: 0.6695 - val_loss: 0.6828\n",
      "Epoch 92/1000\n",
      "799873/799873 [==============================] - 13s - loss: 0.6692 - val_loss: 0.6827\n",
      "Epoch 93/1000\n",
      "799873/799873 [==============================] - 15s - loss: 0.6690 - val_loss: 0.6825\n",
      "Epoch 94/1000\n",
      "799873/799873 [==============================] - 16s - loss: 0.6688 - val_loss: 0.6823\n",
      "Epoch 95/1000\n",
      "799873/799873 [==============================] - 15s - loss: 0.6685 - val_loss: 0.6821\n",
      "Epoch 96/1000\n",
      "799873/799873 [==============================] - 14s - loss: 0.6683 - val_loss: 0.6820\n",
      "Epoch 97/1000\n",
      "799873/799873 [==============================] - 14s - loss: 0.6681 - val_loss: 0.6818\n",
      "Epoch 98/1000\n",
      "799873/799873 [==============================] - 14s - loss: 0.6679 - val_loss: 0.6816\n",
      "Epoch 99/1000\n",
      "799873/799873 [==============================] - 14s - loss: 0.6677 - val_loss: 0.6815\n",
      "Epoch 100/1000\n",
      "799873/799873 [==============================] - 14s - loss: 0.6675 - val_loss: 0.6813\n",
      "Epoch 101/1000\n",
      "799873/799873 [==============================] - 14s - loss: 0.6673 - val_loss: 0.6812\n",
      "Epoch 102/1000\n",
      "799873/799873 [==============================] - 14s - loss: 0.6671 - val_loss: 0.6810\n",
      "Epoch 103/1000\n",
      "799873/799873 [==============================] - 14s - loss: 0.6669 - val_loss: 0.6809\n",
      "Epoch 104/1000\n",
      "799873/799873 [==============================] - 14s - loss: 0.6667 - val_loss: 0.6807\n",
      "Epoch 105/1000\n",
      "799873/799873 [==============================] - 13s - loss: 0.6665 - val_loss: 0.6806\n",
      "Epoch 106/1000\n",
      "799873/799873 [==============================] - 14s - loss: 0.6663 - val_loss: 0.6805\n",
      "Epoch 107/1000\n",
      "799873/799873 [==============================] - 14s - loss: 0.6661 - val_loss: 0.6803\n",
      "Epoch 108/1000\n",
      "799873/799873 [==============================] - 14s - loss: 0.6660 - val_loss: 0.6802\n",
      "Epoch 109/1000\n",
      "799873/799873 [==============================] - 13s - loss: 0.6658 - val_loss: 0.6801\n",
      "Epoch 110/1000\n",
      "799873/799873 [==============================] - 13s - loss: 0.6656 - val_loss: 0.6799\n",
      "Epoch 111/1000\n",
      "799873/799873 [==============================] - 13s - loss: 0.6655 - val_loss: 0.6798\n",
      "Epoch 112/1000\n",
      "799873/799873 [==============================] - 14s - loss: 0.6653 - val_loss: 0.6797\n",
      "Epoch 113/1000\n",
      "799873/799873 [==============================] - 13s - loss: 0.6651 - val_loss: 0.6796\n",
      "Epoch 114/1000\n",
      "799873/799873 [==============================] - 14s - loss: 0.6650 - val_loss: 0.6795\n",
      "Epoch 115/1000\n",
      "799873/799873 [==============================] - 14s - loss: 0.6648 - val_loss: 0.6794\n",
      "Epoch 116/1000\n",
      "799873/799873 [==============================] - 14s - loss: 0.6647 - val_loss: 0.6792\n",
      "Epoch 117/1000\n",
      "799873/799873 [==============================] - 13s - loss: 0.6645 - val_loss: 0.6791\n",
      "Epoch 118/1000\n",
      "799873/799873 [==============================] - 13s - loss: 0.6644 - val_loss: 0.6790\n",
      "Epoch 119/1000\n",
      "799873/799873 [==============================] - 13s - loss: 0.6642 - val_loss: 0.6789\n",
      "Epoch 120/1000\n",
      "799873/799873 [==============================] - 14s - loss: 0.6641 - val_loss: 0.6788\n",
      "Epoch 121/1000\n",
      "799873/799873 [==============================] - 15s - loss: 0.6639 - val_loss: 0.6787\n",
      "Epoch 122/1000\n",
      "799873/799873 [==============================] - 14s - loss: 0.6638 - val_loss: 0.6786\n",
      "Epoch 123/1000\n",
      "799873/799873 [==============================] - 14s - loss: 0.6637 - val_loss: 0.6785\n",
      "Epoch 124/1000\n",
      "799873/799873 [==============================] - 14s - loss: 0.6635 - val_loss: 0.6784\n",
      "Epoch 125/1000\n",
      "799873/799873 [==============================] - 14s - loss: 0.6634 - val_loss: 0.6784\n",
      "Epoch 126/1000\n",
      "799873/799873 [==============================] - 14s - loss: 0.6633 - val_loss: 0.6783\n",
      "Epoch 127/1000\n",
      "799873/799873 [==============================] - 14s - loss: 0.6631 - val_loss: 0.6782\n",
      "Epoch 128/1000\n",
      "799873/799873 [==============================] - 14s - loss: 0.6630 - val_loss: 0.6781\n",
      "Epoch 129/1000\n",
      "799873/799873 [==============================] - 14s - loss: 0.6629 - val_loss: 0.6780\n",
      "Epoch 130/1000\n",
      "799873/799873 [==============================] - 13s - loss: 0.6627 - val_loss: 0.6779\n",
      "Epoch 131/1000\n",
      "799873/799873 [==============================] - 14s - loss: 0.6626 - val_loss: 0.6778\n",
      "Epoch 132/1000\n",
      "799873/799873 [==============================] - 13s - loss: 0.6625 - val_loss: 0.6777\n",
      "Epoch 133/1000\n",
      "799873/799873 [==============================] - 14s - loss: 0.6624 - val_loss: 0.6776\n",
      "Epoch 134/1000\n",
      "799873/799873 [==============================] - 14s - loss: 0.6623 - val_loss: 0.6776\n",
      "Epoch 135/1000\n",
      "799873/799873 [==============================] - 15s - loss: 0.6621 - val_loss: 0.6775\n",
      "Epoch 136/1000\n",
      "799873/799873 [==============================] - 16s - loss: 0.6620 - val_loss: 0.6774\n",
      "Epoch 137/1000\n",
      "799873/799873 [==============================] - 17s - loss: 0.6619 - val_loss: 0.6773\n",
      "Epoch 138/1000\n",
      "799873/799873 [==============================] - 15s - loss: 0.6618 - val_loss: 0.6773\n",
      "Epoch 139/1000\n",
      "799873/799873 [==============================] - 14s - loss: 0.6617 - val_loss: 0.6772\n",
      "Epoch 140/1000\n",
      "799873/799873 [==============================] - 15s - loss: 0.6616 - val_loss: 0.6771\n",
      "Epoch 141/1000\n",
      "799873/799873 [==============================] - 14s - loss: 0.6615 - val_loss: 0.6770\n",
      "Epoch 142/1000\n",
      "799873/799873 [==============================] - 14s - loss: 0.6614 - val_loss: 0.6770\n",
      "Epoch 143/1000\n",
      "799873/799873 [==============================] - 14s - loss: 0.6613 - val_loss: 0.6769\n",
      "Epoch 144/1000\n",
      "799873/799873 [==============================] - 14s - loss: 0.6612 - val_loss: 0.6768\n",
      "Epoch 145/1000\n",
      "799873/799873 [==============================] - 14s - loss: 0.6610 - val_loss: 0.6768\n",
      "Epoch 146/1000\n",
      "799873/799873 [==============================] - 14s - loss: 0.6609 - val_loss: 0.6767\n",
      "Epoch 147/1000\n",
      "799873/799873 [==============================] - 13s - loss: 0.6608 - val_loss: 0.6766\n",
      "Epoch 148/1000\n",
      "799873/799873 [==============================] - 14s - loss: 0.6607 - val_loss: 0.6766\n",
      "Epoch 149/1000\n",
      "799873/799873 [==============================] - 14s - loss: 0.6606 - val_loss: 0.6765\n",
      "Epoch 150/1000\n",
      "799873/799873 [==============================] - 14s - loss: 0.6605 - val_loss: 0.6765\n",
      "Epoch 151/1000\n",
      "799873/799873 [==============================] - 14s - loss: 0.6604 - val_loss: 0.6764\n",
      "Epoch 152/1000\n",
      "799873/799873 [==============================] - 14s - loss: 0.6604 - val_loss: 0.6763\n",
      "Epoch 153/1000\n",
      "799873/799873 [==============================] - 14s - loss: 0.6603 - val_loss: 0.6763\n",
      "Epoch 154/1000\n",
      "799873/799873 [==============================] - 14s - loss: 0.6602 - val_loss: 0.6762\n",
      "Epoch 155/1000\n",
      "799873/799873 [==============================] - 14s - loss: 0.6601 - val_loss: 0.6762\n",
      "Epoch 156/1000\n",
      "799873/799873 [==============================] - 14s - loss: 0.6600 - val_loss: 0.6761\n",
      "Epoch 157/1000\n",
      "799873/799873 [==============================] - 14s - loss: 0.6599 - val_loss: 0.6761\n",
      "Epoch 158/1000\n",
      "799873/799873 [==============================] - 14s - loss: 0.6598 - val_loss: 0.6760\n",
      "Epoch 159/1000\n",
      "799873/799873 [==============================] - 14s - loss: 0.6597 - val_loss: 0.6760\n",
      "Epoch 160/1000\n",
      "799873/799873 [==============================] - 14s - loss: 0.6596 - val_loss: 0.6759\n",
      "Epoch 161/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "799873/799873 [==============================] - 14s - loss: 0.6595 - val_loss: 0.6758\n",
      "Epoch 162/1000\n",
      "799873/799873 [==============================] - 14s - loss: 0.6594 - val_loss: 0.6758\n",
      "Epoch 163/1000\n",
      "799873/799873 [==============================] - 16s - loss: 0.6593 - val_loss: 0.6757\n",
      "Epoch 164/1000\n",
      "799873/799873 [==============================] - 15s - loss: 0.6593 - val_loss: 0.6757\n",
      "Epoch 165/1000\n",
      "799873/799873 [==============================] - 15s - loss: 0.6592 - val_loss: 0.6756\n",
      "Epoch 166/1000\n",
      "799873/799873 [==============================] - 16s - loss: 0.6591 - val_loss: 0.6756\n",
      "Epoch 167/1000\n",
      "301568/799873 [==========>...................] - ETA: 9s - loss: 0.6583"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-366-c418ae794b5a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mnumV\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mEarlyStopping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModelCheckpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'best.h5'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_best_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx_train_user\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnumV\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_train_item\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnumV\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnumV\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx_train_user\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mnumV\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_train_item\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mnumV\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mnumV\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m   1484\u001b[0m                               \u001b[0mval_f\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_ins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_ins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1485\u001b[0m                               \u001b[0mcallback_metrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback_metrics\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1486\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1487\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1488\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch)\u001b[0m\n\u001b[1;32m   1139\u001b[0m                 \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1140\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1141\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1142\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1143\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2101\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2102\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[0;32m-> 2103\u001b[0;31m                               feed_dict=feed_dict)\n\u001b[0m\u001b[1;32m   2104\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 767\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    768\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 965\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1013\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1015\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1016\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1020\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1022\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1023\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1002\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1004\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "user_len = 6040\n",
    "item_len = 3688\n",
    "\n",
    "latent_dim = 10\n",
    "model = get_model(user_len, item_len, latent_dim)\n",
    "numX = 899873\n",
    "numV = 100000\n",
    "callbacks = [EarlyStopping('val_loss', patience=5), ModelCheckpoint('best.h5', save_best_only=True)]\n",
    "model.fit([x_train_user[numV:], x_train_item[numV:]], y_train[numV:], batch_size = 128, epochs = 1000,validation_data = ([x_train_user[:numV], x_train_item[:numV]], y_train[:numV]), callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "testf = open('test.csv','r')\n",
    "testf.readline()\n",
    "x_test = []\n",
    "for line in testf:\n",
    "    x_test.append(line.split(','))\n",
    "x_test = np.array(x_test)\n",
    "x_test_user = x_test[:, 1].astype(int)\n",
    "x_test_item = x_test[:, 2].astype(int)\n",
    "res = model.predict([x_test_user, x_test_item])\n",
    "with open('predict.csv', 'w') as f:\n",
    "    f.write('TestDataID,Rating\\n')\n",
    "    for i, v in  enumerate(res):\n",
    "        f.write('%d,%f\\n' %(i+1, v*y_std+y_mean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "799873/799873 [==============================] - 19s - loss: 14.0777    \n",
      " 99776/100000 [============================>.] - ETA: 0sEpoch 1/1\n",
      "799873/799873 [==============================] - 18s - loss: 14.0775    \n",
      " 99584/100000 [============================>.] - ETA: 0sEpoch 1/1\n",
      "799873/799873 [==============================] - 18s - loss: 14.0773    \n",
      " 99136/100000 [============================>.] - ETA: 0sEpoch 1/1\n",
      "799873/799873 [==============================] - 17s - loss: 14.0771    \n",
      " 99808/100000 [============================>.] - ETA: 0sEpoch 1/1\n",
      "799873/799873 [==============================] - 18s - loss: 14.0768    \n",
      "100000/100000 [==============================] - 5s     \n",
      "Epoch 1/1\n",
      "799873/799873 [==============================] - 19s - loss: 14.0766    \n",
      " 99072/100000 [============================>.] - ETA: 0sEpoch 1/1\n",
      "799873/799873 [==============================] - 17s - loss: 14.0762    \n",
      " 99744/100000 [============================>.] - ETA: 0sEpoch 1/1\n",
      "799873/799873 [==============================] - 17s - loss: 14.0758    \n",
      " 99392/100000 [============================>.] - ETA: 0sEpoch 1/1\n",
      "799873/799873 [==============================] - 18s - loss: 14.0753    \n",
      " 99808/100000 [============================>.] - ETA: 0sEpoch 1/1\n",
      "799873/799873 [==============================] - 17s - loss: 14.0745    \n",
      " 98848/100000 [============================>.] - ETA: 0sEpoch 1/1\n",
      "799873/799873 [==============================] - 17s - loss: 14.0734    \n",
      " 99904/100000 [============================>.] - ETA: 0sEpoch 1/1\n",
      "799873/799873 [==============================] - 18s - loss: 14.0717    \n",
      " 99168/100000 [============================>.] - ETA: 0sEpoch 1/1\n",
      "799873/799873 [==============================] - 18s - loss: 14.0692    \n",
      " 99072/100000 [============================>.] - ETA: 0sEpoch 1/1\n",
      "799873/799873 [==============================] - 17s - loss: 14.0652    \n",
      " 99584/100000 [============================>.] - ETA: 0sEpoch 1/1\n",
      "799873/799873 [==============================] - 18s - loss: 14.0590    \n",
      " 98880/100000 [============================>.] - ETA: 0sEpoch 1/1\n",
      "799873/799873 [==============================] - 19s - loss: 14.0492    \n",
      " 99360/100000 [============================>.] - ETA: 0sEpoch 1/1\n",
      "799873/799873 [==============================] - 18s - loss: 14.0337    \n",
      " 99136/100000 [============================>.] - ETA: 0sEpoch 1/1\n",
      "799873/799873 [==============================] - 17s - loss: 14.0094    \n",
      " 99776/100000 [============================>.] - ETA: 0sEpoch 1/1\n",
      "799873/799873 [==============================] - 18s - loss: 13.9709    \n",
      " 99936/100000 [============================>.] - ETA: 0sEpoch 1/1\n",
      "799873/799873 [==============================] - 18s - loss: 13.9103    \n",
      " 99328/100000 [============================>.] - ETA: 0sEpoch 1/1\n",
      "799873/799873 [==============================] - 17s - loss: 13.8154    \n",
      " 99136/100000 [============================>.] - ETA: 0sEpoch 1/1\n",
      "799873/799873 [==============================] - 18s - loss: 13.6683    \n",
      " 99456/100000 [============================>.] - ETA: 0sEpoch 1/1\n",
      "799873/799873 [==============================] - 18s - loss: 13.4434    \n",
      " 99072/100000 [============================>.] - ETA: 0sEpoch 1/1\n",
      "799873/799873 [==============================] - 16s - loss: 13.1078    \n",
      " 99168/100000 [============================>.] - ETA: 0sEpoch 1/1\n",
      "799873/799873 [==============================] - 16s - loss: 12.6244    \n",
      "100000/100000 [==============================] - 3s     \n",
      "Epoch 1/1\n",
      "799873/799873 [==============================] - 13s - loss: 11.9618    \n",
      " 98976/100000 [============================>.] - ETA: 0sEpoch 1/1\n",
      "799873/799873 [==============================] - 13s - loss: 11.1138    \n",
      " 99232/100000 [============================>.] - ETA: 0sEpoch 1/1\n",
      "799873/799873 [==============================] - 13s - loss: 10.1182    \n",
      " 98880/100000 [============================>.] - ETA: 0sEpoch 1/1\n",
      "799873/799873 [==============================] - 13s - loss: 9.0592    \n",
      " 98816/100000 [============================>.] - ETA: 0sEpoch 1/1\n",
      "799873/799873 [==============================] - 13s - loss: 8.0378    \n",
      " 99648/100000 [============================>.] - ETA: 0s"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nmodel = get_model(user_len, item_len, 10) \\nfor i in range(10):\\n    model.fit([x_train_user[numV:], x_train_item[numV:]], y_train[numV:], batch_size = 128, epochs = 1)\\n    score = model.evaluate([x_train_user[:numV], x_train_item[:numV]], y_train[:numV])\\n    ld10.append(score)\\nmodel = get_model(user_len, item_len, 50)\\nfor i in range(10):\\n    model.fit([x_train_user[numV:], x_train_item[numV:]], y_train[numV:], batch_size = 128, epochs = 1)\\n    score = model.evaluate([x_train_user[:numV], x_train_item[:numV]], y_train[:numV])\\n    ld15.append(score)\\nmodel = get_model(user_len, item_len, 100)\\nfor i in range(10):\\n    model.fit([x_train_user[numV:], x_train_item[numV:]], y_train[numV:], batch_size = 128, epochs = 1)\\n    score = model.evaluate([x_train_user[:numV], x_train_item[:numV]], y_train[:numV])\\n    ld20.append(score)\\n'"
      ]
     },
     "execution_count": 415,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_len = 6040\n",
    "item_len = 3688\n",
    "\n",
    "latent_dim = 10\n",
    "model = get_model(user_len, item_len, latent_dim)\n",
    "numX = 899873\n",
    "numV = 100000\n",
    "#ld5 = []\n",
    "#ld10 = []\n",
    "ld15 = []\n",
    "ld20 = []\n",
    "model = get_model(user_len, item_len, 10)\n",
    "for i in range(30):\n",
    "    model.fit([x_train_user[numV:], x_train_item[numV:]], y_train[numV:], batch_size = 128, epochs = 1)\n",
    "    score = model.evaluate([x_train_user[:numV], x_train_item[:numV]], y_train[:numV])\n",
    "    ld15.append(score)\n",
    "'''\n",
    "model = get_model(user_len, item_len, 10) \n",
    "for i in range(10):\n",
    "    model.fit([x_train_user[numV:], x_train_item[numV:]], y_train[numV:], batch_size = 128, epochs = 1)\n",
    "    score = model.evaluate([x_train_user[:numV], x_train_item[:numV]], y_train[:numV])\n",
    "    ld10.append(score)\n",
    "model = get_model(user_len, item_len, 50)\n",
    "for i in range(10):\n",
    "    model.fit([x_train_user[numV:], x_train_item[numV:]], y_train[numV:], batch_size = 128, epochs = 1)\n",
    "    score = model.evaluate([x_train_user[:numV], x_train_item[:numV]], y_train[:numV])\n",
    "    ld15.append(score)\n",
    "model = get_model(user_len, item_len, 100)\n",
    "for i in range(10):\n",
    "    model.fit([x_train_user[numV:], x_train_item[numV:]], y_train[numV:], batch_size = 128, epochs = 1)\n",
    "    score = model.evaluate([x_train_user[:numV], x_train_item[:numV]], y_train[:numV])\n",
    "    ld20.append(score)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10.686720119171143, 8.3957363078308109, 6.8029138369750974, 5.6686955393218996, 4.8432666324615479, 4.2299533554077149, 3.7653656306076049, 3.4070275932312013, 3.1257731739807131, 2.9013227482986452, 2.7193597786331178, 2.569748448524475, 2.4449091252517698, 2.339464044456482, 2.2493081541061399, 2.1714019312286377, 2.1033243591308595, 2.0433903137397764, 1.9901394667816161, 1.9424879794120788, 1.8995409893608093, 1.8606028100013734, 1.8251090124320983, 1.7925907040405273, 1.7626647717285155, 1.7350271672058106, 1.7094115812301636, 1.6855737604713439, 1.6633490307617187, 1.6425400475120544]\n",
      "[11.60364644744873, 9.838163272094727, 8.5280201902771005, 7.5272343363952636, 6.7428880114746095, 6.1152216953277589, 5.603760366668701, 5.1804342797088623, 4.8253426918029785, 4.5240985494232175, 4.2659380440902712, 4.0426521427536013, 3.8480089997863769, 3.6771875263595581, 3.5262692609024047, 3.3921505850601195, 3.2723271812438965, 3.1647242307662964, 3.0676971829032897, 2.9798365675735474, 2.8999754008865355, 2.827079227180481, 2.7603756068801881, 2.6991715497970583, 2.6427989700317385, 2.5907886889266969, 2.5426666489791869, 2.498028656311035, 2.4565149528694152, 2.4178651698875426]\n",
      "[14.064977428588866, 14.06499000793457, 14.064996453857422, 14.064990146484375, 14.06496046142578, 14.064889543151855, 14.064752285156249, 14.064507684936524, 14.064092043762207, 14.063405054626465, 14.062286195983887, 14.060480670166015, 14.057581323547364, 14.052943116455078, 14.045534354553222, 14.033732220458985, 14.014951485595702, 13.985154608764649, 13.938022329101562, 13.863842234802247, 13.747995347595214, 13.569115979919433, 13.298279422607422, 12.899615637817384, 12.33668910583496, 11.587039443359375, 10.663432863464356, 9.6280532229614266, 8.5794136361694342, 7.6111053752136231]\n"
     ]
    }
   ],
   "source": [
    "print(ld5)\n",
    "print(ld10)\n",
    "print(ld15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEKCAYAAAAfGVI8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3Xd4VFX+x/H3NwkQUggJhB6qqLSAGFcBQRQbiCKKIk1A\nBBTruotlxbo/6+paAAVUFFwLCKwFV6WoKEUUkB4FASH00BNCCEnO748zaZCEScjMnfJ9Pc995s6d\ncr/XecyHe8+554gxBqWUUsErxOkClFJKOUuDQCmlgpwGgVJKBTkNAqWUCnIaBEopFeQ0CJRSKshp\nECilVJDTIFBKqSCnQaCUUkEuzOkC3FGzZk3TuHFjp8tQSim/snz58n3GmPjTvc8vgqBx48YsW7bM\n6TKUUsqviMhWd96nl4aUUirIaRAopVSQ0yBQSqkgp0GglFJBToNAKaWCnAaBUkoFOY8FgYhMFpG9\nIrK2mNf+JiJGRGp6av9KKaXc48n7CN4DxgFTC28UkQTgSmCbB/cNwOzZ8PPPnt5LxRHxrX2XVE/h\n7Se/p6TX8tZLez1vOfl5SdtCQk7dVvi1vNfz1k9+nrceGlqwnPy88BIWBpUqlfxYeD00tPT/3kr5\nEo8FgTHmBxFpXMxLrwAPAp95at95vv4a3njD03upGDp1dGAJDYUqVYou4eGnPq9aFSIjISrKLnnr\nxW2LjYW4OKhRw35WqYri1TuLRaQXsMMYs0pO889fERkBjABo2LBhufY3bpxdVNmVFEyFt5/8npJe\ny1sv7fW85eTnJW3LW3JzS99e+DFvOfl5Tk7RpbhtOTmQnW2XEycKHguvF952/HjRJTPz1OdHj8K+\nffYxPb3gMTf39L9P1ao2FPKCIW89Lg5q1YKEhIKlbl09Q1Gl81oQiEgE8A/sZaHTMsZMAiYBJCUl\n6b+Xvcydy0Kq4hlTEBKFwyE9HQ4ehP374cABuxRe/+23gm0nThT9ztBQGwaFwyEhARo2hBYt4Kyz\n7CUtFby8+fM3A5oAeWcDDYAVIvIXY8xuL9ahlM8Ssf/ar1oVapajK4UxcOgQpKTA9u32sfDy66/w\n+ec2bPJUqQItW0KbNkWXunU1+IOF14LAGLMGqJX3XET+BJKMMfu8VYNSgU7EtiXExkJiYvHvMcae\nOWzdCuvWwZo1dpk3D6YW6toRFwetW9tQSEqCyy+HBg28cxzKuzwWBCLyEdAVqCki24EnjDHveGp/\nSin3iNizjZo14fzzi7524EBBMOQtU6fC+PH29RYt4Mor4Yor4JJLbCO28n9i/KC7SlJSktFhqJVy\nRm4urF0Lc+fCnDnwww/20lKlStCxY0EwtG+vjdK+RkSWG2OSTvs+DQKlVFlkZsLChQXBsHKl3R4X\nB926wa23QvfuGgq+QINAKeUVe/fa9oW5c+F//7PPGzWCESNg2DCoXdvpCoOXu0GgYw0ppc5IrVrQ\nvz+8+67tqTR9OjRtCo8+arup3nILLFigN036Mg0CpVSFqVQJbroJvv0WkpNh1Ch7h3/XrrYH0rhx\ncPiw01Wqk2kQKKU84txz4dVXYedOeOcdiIiAe+6BevXsZaO1pwxHqZyiQaCU8qiICLjtNvjlF7vc\ncgv85z/Qti3ce6+eIfgCDQKllNckJdmzg5QUGDnSXio65xwbDNqG4BwNAqWU19WoYUcG/vlnO+bR\noEG2HUEvFzlDg0Ap5ZikJPjpJ5g40YZAu3bw979DWprTlQUXDQKllKNCQmzj8e+/w5Ah8PLLtqF5\n2jS9XOQtGgRKKZ9Qsya8/TYsWWJvQrvlFjt8xe+/O11Z4NMgUEr5lIsusr2Lxo2zj23awKRJTlcV\n2DQIlFI+JzQU7rrLng1062Z7GI0e7d7sbarsNAiUUj6rdm344gsbCi+9BDfeaGdtUxVLg0Ap5dPC\nwuxlotdes7OrXXKJvVtZVRwNAqWUX7j3XvjsMzs/84UXwqpVTlcUODQIlFJ+o2dPOxeCMXDxxXbY\na3XmNAiUUn6lXTtYuhSaN4drr7WXjdSZ0SBQSvmd+vXtlJk9e9oRTe+9F3JynK7Kf2kQKKX8UlQU\nzJoFDzwAY8dCr146NEV5aRAopfxWaKgdkuKNN+wEON26affS8tAgUEr5vTvvhJkzYfly6NsXsrOd\nrsi/eCwIRGSyiOwVkbWFtv1LRH4TkdUi8l8Rqe6p/SulgkuvXvbM4Msv7Q1oOmCd+zx5RvAecPVJ\n2+YCrY0xicAG4BEP7l8pFWRGjoRHHrFjEz33nNPV+A+PBYEx5gfgwEnb5hhj8k7afgIaeGr/Sqng\n9MwzMGAAPPqonflMnV6Yg/u+DZjm4P6VUgFIBCZPtsNQ3HYb1K1rG5FVyRxpLBaRR4Fs4INS3jNC\nRJaJyLLU1FTvFaeU8nuVK9uupWefDTfcAGvWOF2Rb/N6EIjIEKAnMMCYkptzjDGTjDFJxpik+Ph4\nr9WnlAoM1avDV1/Z+w26d4ft252uyHd5NQhE5GrgQeA6Y0yGN/etlAo+CQk2DI4cgR494PBhpyvy\nTZ7sPvoRsAQ4R0S2i8gwYBwQDcwVkZUiMsFT+1dKKYDERHuPQXKync8gK8vpinyPxxqLjTH9itn8\njqf2p5RSJbniCjsf8pAhcPvtMGWKbVRWVkDfWbxx/0ZmrJ/hdBlKKR8weDD885/w/vvw2GNOV+Nb\nAjoI/rX4Xwz+dDBpx3UkKqWUvbfg9tvtvQYffeR0Nb4joINgSLshZJzIYGbyTKdLUUr5ABF4803o\n2BHuuAP+/NPpinxDQAdBhwYdaB7XnPdWvud0KUopHxEWVnDH8cCBOkAdBHgQiAiD2w5mwdYFbDm4\nxelylFI+okkTO0DdokU6JhEEeBAADGo7CEGYumqq06UopXzIgAHQvz889RQsWeJ0Nc4K+CBoGNOQ\ny5pcxpRVU8g1uU6Xo5TyIW+8YW86GzDA3nQWrAI+CMA2Gm85tIWF2xY6XYpSyofExNj2gq1b4e67\nna7GOUERBL3P7U105WhtNFZKnaJTJ3tfwfvvB2+X0qAIgsjKkdzU8iY+Wf8JR7N0QlOlVFFjxtgu\npXfeac8Ogk1QBAHYy0PpWenMSp7ldClKKR+T16XUmODsUho0QXBxw4tpGtuU91a953QpSikflNel\ndOHC4OtSGjRBkHdPwXdbvmProSA891NKndaAAXYJti6lQRMEALe2vRWD4f3V7ztdilLKR40fH3xd\nSoMqCBpXb0zXxl2ZsmoKpUyOppQKYoW7lN5zj9PVeEdgB8Err8D11xfZNKTtEP448AeLUxY7VJRS\nytfldSmdOjU4upQGdhBkZsJnn8Hu3fmbbmx5I5GVIvWeAqVUqcaMgQ4d4K67YM8ep6vxrMAOgh49\n7OPXX+dviqocRZ+WfZi2bhoZJ3TaZKVU8cLCYPJkOHoU7r/f6Wo8K7CDIDER6tWD//2vyOYh7YaQ\nlpXGp7996lBhSil/cO659szg449P+TMSUAI7CESge3eYMwdOnMjf3KVRFxpXb6yXh5RSp/XQQ9Cq\nlZ3IJi1AJzsM7CAAe3no8OEinYJDJIRbE29l3uZ5bD+y3cHilFK+rnJleOst2L7dnh0EosAPgssv\ntxf7vvqqyOb8ewpW6T0FSqnSdegAo0bB2LGwdKnT1VS8wA+CatXg4otPucDXLK4ZnRt25r1V7+k9\nBUqp03r2WdvkOHx4kSvNAcFjQSAik0Vkr4isLbQtTkTmishG12Osp/ZfRI8esHq1PbcrZEi7IWzY\nv4GlOwIw4pVSFapaNTsW0Zo18K9/OV1NxfLkGcF7wNUnbXsYmG+MaQ7Mdz33vGK6kQLc1PImIipF\naKOxUsot110HffrA00/Dhg1OV1NxPBYExpgfgAMnbe4FTHGtTwGuxxtatrSDh5x0eSi6SjQ3tLiB\nj9d+zLETx7xSilLKv73+OoSHw8iRdtjqQODtNoLaxphdrvXdQG2v7FXEnhXMnQtZWUVeGtJ2CIeP\nH+bz3z/3SilKKf9Wt669NPT99/aGs0DgWGOxsS20JeapiIwQkWUisiw1NfXMd9ijB6Sn28HGC7m0\nyaUkVEvQeQqUUm4bNgy6dIG//73ICDZ+y9tBsEdE6gK4HveW9EZjzCRjTJIxJik+Pv7M93zZZbZD\n8EndSEMkhFvb3sqcTXPYmbbzzPejlAp4ISEwaRIcOwb33ed0NWfO20HwOTDYtT4Y+Mxre46KshFe\nzH3ig9sOJtfkMnHZRK+Vo5Tyb+ecY28wmz4dvvjC6WrOjCe7j34ELAHOEZHtIjIMeB64QkQ2Ape7\nnntPjx6wfv0ps1M3r9Gc3uf25tWlr3Lw2EGvlqSU8l8PPgitW9ubzfx5EhtP9hrqZ4ypa4ypZIxp\nYIx5xxiz3xjTzRjT3BhzuTHm5F5FntW9u3086fIQwJNdn+TI8SO88tMrXi1JKeW/8oaf2LEDHn3U\n6WrKL/DvLC7snHPsDNXFXB5KrJ3IjS1u5NWfXuXAMe/mk1LKf110kZ2zYPx4+Oknp6spn+AKgrxu\npPPn20lrTvLEJU+QlpXGv5f824HilFL+6tlnoX59/x1+IriCAGwQZGTAjz+e8lKb2m24qeVNvLb0\nNfZn7HegOKWUP4qOhnHjYO1aePllp6spu+ALgq5doUqVEmeZePySxzmadVTPCpRSZdKrF/TuDU89\nBZs2OV1N2QRfEEREwKWXlhgErWu15qZWN/H6z6+zL2Ofl4tTSvmzsWOhUiW4807/Gn4i+IIA7OWh\nDRtKjO0nLnmCo1lHeXmxH57jKaUcU7++bS+YOxc+/NDpatwXnEFQSjdSgJbxLenbui9jfx5L6tEK\nGN5CKRU07rwTLrwQ/vpXOOAnHRCDMwjOOguaNy91NurHuzxOxokMXlr8khcLU0r5u9BQO/zEgQMw\nerTT1bgnOIMA7OWh776zg4UUo0V8C/q16ce4X8ax92iJQyIppdQpEhPhb3+zo5MuWOB0NacX3EGQ\nmWnHki3B410eJzM7k38tCrDpiJRSHvfEE/b+1ZEj4fhxp6spXfAGQZcutgdRKZeHzql5Dv3b9Gf8\nL+P1rEApVSYREfDmm/D77/Dcc05XU7rgDYLwcDs09f/+V2o/r8e6PMbxnOO8uOhFLxanlAoEV10F\n/frZIPjtN6erKVnwBgHYy0ObN5c6+ejZNc5mYOJA3vjlDXanB8AMFEopr3rlFXt2MHIk5OY6XU3x\ngjsITtONNM+YzmPIysnSswKlVJnVrm2ntvzhB3j3XaerKV5wB0HjxtCiRantBGDnKxiYOJA3l73J\nrrRdpb5XKaVOdttt0Lmz7U661webG4M7CMBeHlqwwM5nXIrHujzGiZwTvLDoBS8VppQKFCEhMHGi\n/TPz1786Xc2pNAi6d4esLHtPQSmaxTXj1ra3MmHZBJ3bWClVZi1awCOP2KEnvvnG6WqK0iC4+GI7\nn/FpLg8BjOkyhhyTw/MLvTvDplIqMDzyiJ0fa+RISEtzupoCGgRVqsDll5+2GylA09imDG03lAnL\nJrBmzxovFaiUChTh4fZu45QUe+exr9AgANtOsG2bndj+NJ7r9hyxVWMZ+tlQsnOzvVCcUiqQdOwI\nf/+7nev4NB0WvUaDAAq6kc6Ycdq31oiowfge41m+a7kOPaGUKpenn4ZWrWDYMN8YodTtIBCRi0Vk\nqGs9XkSaeK4sL2vQwN4COGmSWxOO9mnZhz4t+/DkgidJTk32QoFKqUBSpQq8/z6kpsI99zhdjZtB\nICJPAA8Bj7g2VQL+46miHDFqFOzcCV984dbbx/cYT3TlaG77/DZycnM8XJxSKtCcdx48/rjtReTG\nxQiPcveMoDdwHXAUwBizE4gu705F5K8isk5E1orIRyISXt7vqjDXXAMNG8Ibb7j19lqRtRjbfSw/\nbf+JV3961cPFKaUC0cMPQ1IS3HEH7NnjXB3uBkGWMcYABkBEIsu7QxGpD9wLJBljWgOhwC3l/b4K\nExpq+3TNn+/26FC3tL6F6865jjHfjWHD/pLHK1JKqeJUqgRTptgbzUaMcG6eY3eDYLqITASqi8hw\nYB7w1hnsNwyoKiJhQATgG3doDRtmf5kJE9x6u4gw4ZoJhIeFM+zzYeQaHx1RSinls1q2tPMcf/45\nTJ3qTA1uBYEx5iVgBjATOAd43Bgztjw7NMbsAF4CtgG7gMPGmDnl+a4KV7s29OkD770HR4+69ZG6\n0XV59apXWbhtIeN/Hu/Z+pRSAem+++xYRPfea+8x8DZ3G4sjgW+NMaOxZwJVRaRSeXYoIrFAL6AJ\nUA+IFJGBxbxvhIgsE5FlqalenEB+1Cg4fBg++sjtj9za9la6n9Wdh+c/zOaDmz1YnFIqEIWG2n9/\n5uTYAeq8fYnI3UtDPwBVXNf3vwYGAe+Vc5+XA1uMManGmBPALKDjyW8yxkwyxiQZY5Li4+PLuaty\n6NQJ2rSB8ePd/jVEhIk9JxIqodz++e16iUgpVWZNm8LLL8O8eXZmM29yNwjEGJMB3AC8aYy5CWhV\nzn1uAy4SkQgREaAb4Dud8UXsWcHKlbB0qdsfS4hJ4OUrX+a7P7/jreVn0nyilApWI0bYW5pGj4Y/\n/vDeft0OAhHpAAwAvnRtCy3PDo0xS7HtDSuANa4aJpXnuzxmwACIjna7K2me29vfTrcm3Rg9dzTb\nDm/zUHFKqUAlAu+8A5Urw5Ah9lKRN7gbBPcBDwOzjDHrXHcVf1venRpjnjDGnGuMaW2MGWSMOV7e\n7/KI6Gi49VaYNg327XP7YyLC29e9Ta7JZfgXwzFO9QVTSvmt+vVh7FhYtAj+/W/v7NPdIMgAcoF+\nIrIa+By41GNV+YI777TzFEyeXKaPNa7emBcuf4E5m+bw7kofnZdOKeXTBgyA3r1hzBhYt87z+3M3\nCD4AJmPbCK4FeroeA1erVnDJJfaegjKen915wZ1c0ugSHvjmAbYf2e6hApVSgUrE/um56CK3hj87\nY+4GQaox5gtjzBZjzNa8xaOV+YJRo2DLljJPJxQiIbx93dtk52Zzw7QbOHbimIcKVEoFqlq17Cy6\n7dp5fl/uBsETIvK2iPQTkRvyFo9W5guuvx7q1ClzozHAWXFn8cENH7Bs5zKGfDZEu5QqpXyWu0Ew\nFGgHXI29JJR3eSiwVa4Mw4fb2cu2bCnzx3ud24vnL3+e6eum89T3T3mgQKWUOnPuBsEFrpu7Bhtj\nhrqW2zxama8YMQJCQmDixHJ9fHTH0QxtN5Snf3iaj9a4f7eyUkp5i7tBsFhEWnq0El/VoAFcd53t\n3JuZWeaPiwgTek6gS6MuDP1sKD9t/8kDRSqlVPm5GwQXAStF5HcRWS0ia1zdSIPDqFH2foJyzh5R\nObQyM2+eSf1q9en1cS+2Hgr8dnallP8Qd256EpFGxW33Vs+hpKQks2zZMm/sqni5udCiBdSoAYsX\nl/trklOT6fBOBxrGNGTRbYuIrlLuuX2UUuq0RGS5MSbpdO9zdxjqrcUtZ16mnwgJsTeYLVkCv/5a\n7q9pEd+C6TdNZ33qevrP6q9TXCqlfILbk9cHvcGDoWrVMx4W8MpmV/J699eZvWE2D817qIKKU0qp\n8tMgcFdsLPTvDx98AIcOndFXjbpgFHdfcDcvL3mZt1e8XUEFKqVU+WgQlMWoUZCRUSHzyb1y9Stc\n1ewq7vzyTr7b8l0FFKeUUuWjQVAW7dvDhRfCuHFnPABIWEgY0/pM4+waZ3Pj9BvZuH9jBRWplFJl\no0FQVv/4B2zcWCFTCMWEx/BFvy8IDQml+wfddQ4DpZQjNAjK6tpr4cor4YknoALmUm4a25TZ/Waz\nL2Mfnd/tzB8HvDgtkVJKoUFQdiLwyiuQlgaPPVYhX3lhgwv5bvB3ZJzIoPO7nVm31wsDkCullIsG\nQXm0bAl33w2TJtm5jSvAeXXPY8GQBQjCJe9dwvKdyyvke5VS6nQ0CMrrySftncb33gsVNCVly/iW\n/Dj0R6IqR3HZ1MtYtG1RhXyvUkqVRoOgvKpXh2eegR9/hOnTK+xrm8U148ehP1Inqg5X/udK5m2e\nV2HfrZRSxdEgOBPDhsF558Ho0fb+ggqSEJPAD0N+4Ky4s7jmw2v44vcvKuy7lVLqZBoEZyI0FF57\nDVJS4MUXK/Sra0fV5rvB39GuTjtumH4D09ZOq9DvV0qpPBoEZ6pzZ+jbF154AbZW7Dh8cVXjmDdo\nHh0TOtJvZj8m/zq5Qr9fKaXAoSAQkeoiMkNEfhORZBHp4EQdFeZf/7LdSkePrvCvjq4SzVcDvuKq\ns65i2OfDeH3p6xW+D6VUcHPqjOA14GtjzLlAWyDZoToqRkICPPwwfPIJfP99hX99RKUIPu37Kb3P\n7c19X9/H6Dmjyc7NrvD9KKWCk1sT01ToDkVigJVAU+Pmzh2fmMYdx47ZyWtiYmD5cggLq/BdZOdm\nc99X9/HGsjfo0qgL0/pMo05UnQrfj1IqMFToxDQVrAmQCrwrIr+KyNsiEulAHRWralV46SVYvRre\nessjuwgLCWP8NeN5v/f7/LLjF9pPbM/CbQs9si+lVPBwIgjCgPbAm8aY84CjwMMnv0lERojIMhFZ\nlloBY/p4xY03QteuMGYMHDjgsd0MTBzI0tuXElk5kq7vdeWVJa/g7TM7pVTgcCIItgPbjTFLXc9n\nYIOhCGPMJGNMkjEmKT4+3qsFlpuI7U566JC989iD2tRuw7Lhy7j2nGt5YM4D9J3Rl7TjaR7dp1Iq\nMHk9CIwxu4EUETnHtakbsN7bdXhMYiKMHAlvvAFr13p0VzHhMcy6eRYvXP4CM5Nn8pe3/8L61MD5\nT6mU8g6neg3dA3wgIquBdsCzDtXhGf/8J1SrBvffX2HjEJVERHiw04PMv3U+B44d4C9v/UVvPlNK\nlYkjQWCMWem67JNojLneGHPQiTo8pkYNePppmD8fPvrIK7vs2rgrK0asoG2dttwy8xbu//p+snKy\nvLJvpZR/0zuLPeWOO6BjRxg+3PYk8oL61erz/eDvue/C+3ht6Wt0ebeLXipSSp2WBoGnhIXBjBl2\nlNLevT3ai6iwSqGVePXqV5nWZxobD2yk3YR2PPX9UxzPPu6V/Sul/I8GgSfVrQszZ9pB6fr1g5wc\nr+365lY3k3xXMn1a9uHJBU/SflJ7lqQs8dr+lVL+Q4PA0y66CMaPhzlz7P0FXlQrshYf3vghs/vN\nJu14Gp0md+Ler+7VbqZKqSI0CLxh+HDbpfT55+14RF52zdnXsG7UOu664C7G/TyO1m+25quNX3m9\nDqWUb9Ig8JbXXoMOHWDIEFizxuu7j64SzdgeY1l420IiK0XS48MeDJg1gNSjfnLXtlLKYzQIvKVK\nFdteEBNjG48POtNjtmNCR34d+StPXvIkn6z7hBbjW/Cf1f/RISqUCmIaBN6U13i8bZvXG48LqxJW\nhSe6PsGvI3+leY3mDPrvIC5+92IWbVvkSD1KKWdpEHhbhw4wbhx88w089pijpbSq1YqFQxcyqeck\nthzcwsXvXkyvj3uxbu86R+tSSnmXBoETRoywy3PP2XsNHBQaEsrw84fzx71/8Oxlz/L9n9+TOCGR\nYZ8NI+VwiqO1KaW8w+sT05SHX0xMU1bHj8Oll9q7jn/6CVq3droiAPZn7OfZH59l3C/jCJEQ7vnL\nPTx88cPEVY1zujSlVBn58sQ0Cmzj8YwZEB0N11/vWOPxyWpE1ODlq15mw90buLnVzby0+CWavd6M\nFxe9yLETx5wuTynlARoETqpXr6Dx+Oab7XSXPqJR9UZMuX4KK+9YSaeETjw07yGaj23OpOWTyMzO\ndLo8pVQF0iBwWseOMGmSHam0e3c4csTpiopIrJ3I7P6z+X7w9zSo1oCRs0fS+NXGPPvjsxw85htn\nMUqpM6NB4AuGDIEPPoBFi2y7gQ9OzXlJ40tYMmwJ8wbNo12ddjz67aMkvJLA/V/fz9ZDW50uTyl1\nBjQIfEW/fvDpp7B+PXTubAeq8zEiQrem3fh64NesumMVN7S4gfG/jKfZ683oP7M/v+761ekSlVLl\noEHgS665xg5Ot2sXdOoEv//udEUlSqydyNTeU9l872buv+h+Zm+YTftJ7bl86uV888c3eqeyUn5E\ng8DXdO4M338PmZl2fcUKpysqVUJMAi9d+RIpf03hhctfIHlfMld/cDVtJ7Rl4rKJHDnuW20eSqlT\naRD4ovPOg4ULoWpV22bwww9OV3RaMeExPNjpQbbct4V3e72LiHDHl3dQ9+W6DP1sKIu2LdKzBKV8\nlN5Q5stSUuDKK+HPP+09B9dc43RFbjPG8MvOX3h7xdt8tPYj0rPSaVGzBbe3v51BiYOIj4x3ukSl\nAp67N5RpEPi6ffvg6qth1SqYMgX693e6ojJLz0pn+rrpvL3ibZZsX0KlkEpcf+71DG8/nG5NuxEi\nemKqlCdoEASSI0fguuvsJaJx42DUKKcrKre1e9fyzop3mLp6KgeOHaBRTCOGtBtC31Z9aRHfwuny\nlAooGgSB5tgxuOUW+PxzuPtuePFF24bgp45nH+fT3z7lrRVv8e2WbzEY2tRqQ99Wfenbui9nxZ3l\ndIlK+T2fDwIRCQWWATuMMT1Le68GgUt2NoweDa++Ci1awIcfQrt2Tld1xnam7WTG+hlMWzeNxSmL\nATivznn0bdWXm1vdTJPYJg5XqJR/8ocgeABIAqppEJTR3LkweLBtP3jmGfjb3yAkMK6zpxxO4ZP1\nnzBt3TR+3vEzABfUu4C+rfpyU6ubaBjT0OEKlfIfPh0EItIAmAI8AzygQVAO+/fbOQ1mzYKuXWHq\nVEhIcLqqCrXl4Jb8UFixy95P0b5ue3o278k1Z19DUr0kbWhWqhS+HgQzgOeAaODvxQWBiIwARgA0\nbNjw/K1bdTybUxgD770H994LoaEwYYJtRwhAG/dvZMb6GXy58UuWbF9CrsmlVmQtejTvQc/mPbmi\n2RVUq1LN6TKV8ik+GwQi0hPoYYwZJSJdKSEICtMzgtPYtAkGDYIlS2DAABg/HmJinK7KY/Zn7Ofr\nP75m9sbZfP3H1xzKPESlkEp0adSFa5pfQ8+ze9K8RnOny1TKcb4cBM8Bg4BsIByoBswyxgws6TMa\nBG7IzrYF6nm4AAAQ0klEQVRTXz71FNSvD++/D126OF2Vx2XnZrM4ZTFfbviS2Rtnsz51PQDNYpvR\nrUk3ujXtRtfGXakVWcvhSpXyPp8NgiI71zOCird0KQwcaM8SHngAHnssoM8OTrbl4Ba+3PglczbN\nYcHWBfljHbWp1YZuTbpxWZPL6NKoCzHhwfPfRAUvDYJglp5uQ+Ctt6BGDRsGd9xhp8cMItm52Szf\nuZxvt3zLt39+y8JtC8nMziRUQkmql8RlTS7jsiaX0aFBByIrRzpdrlIVzi+CwF0aBOW0YgU89BDM\nmwdNmtiupn37BkxX07LKzM7kp+0/8e2Wb5m/ZT4/7/iZ7NxsQiWUdnXa0TGhI50SOtExoSMJMYHV\nA0sFJw0CVWDOHHjwQTteUfv29q7kbt2crspxacfTWLhtIYtSFrE4ZTFLdywl40QGAAnVEooEQ9s6\nbQkLCXO4YqXKRoNAFZWba+9EHjMGtm6Fq66CF16Atm2drsxnnMg5weo9q1mUssgu2xaxI20HABGV\nIrig3gVcUO8CkuolkVQviaaxTRERh6tWqmQaBKp4mZnwxhvwf/8Hhw7ZhuV//hMaNXK6Mp+07fA2\nFqcsZtG2Rfy882dW7l5JVk4WALHhsZxf7/wi4ZBQLUHDQfkMDQJVukOH4Pnn4bXX7I1pAwfCPffo\nGcJpZOVksXbvWpbtXJa/rNm7huzcbADiI+JJqpdEuzrtSKydSNvabWleo7leVlKO0CBQ7klJgWef\ntXMdHDtmp8e85x7o3RvC9I+XOzKzM1m9Z3WRcEjel5wfDuFh4bSKb5UfDIm1E0msnUiNiBoOV64C\nnQaBKpuDB2HyZHtX8pYt0KAB3HknDB8O8TqbWFll5WSRnJrM6j2rWbVnlV12ryI1IzX/PfWj69Om\ndhta1mxJy3i7tIhvQfXw6g5WrgKJBoEqn5wc+PJLGDvWdjutUsWOX3TPPXD++U5X5/d2p++24bDb\nhsO61HX8tu83MrMz899TN6puQTDUbJG/XjOiprY/qDLRIFBnLjnZzog2ZQocPQodOtizhOuvh+ho\np6sLGDm5Ofx56E+S9yWzPnV9/pK8L5n0rPT891UPr87ZNc7m7Bpn0zyueZH16Cr6e6hTaRCoinP4\nsB3ldNw4+OMPCA+HHj3szWk9e0JEhNMVBiRjDNuPbM8Pho0HNrJh/wY27N9AypGUIu+tE1UnPxSa\nxzWnaWzT/CW2aqxDR6CcpkGgKl5urh3h9OOP4ZNPYM8eGwLXXmtDoXt3GxLK4zJOZLDpwKYi4ZC3\nvvfo3iLvrR5ePT8UmsU2y19vUr0JCTEJVA6t7NBRKE/TIFCelZMDP/wA06bBzJl2trToaHvZqG9f\nuOIKqKx/YJyQdjyNLYe2sPngZjYd2MTmg5vZfGgzmw9u5s9Df+bfBwEgCPWi69GoeiMaxbiW6kUf\ndRwm/6VBoLwnOxu+/daGwqxZ9h6F6tXt3ctXXQVXXmmHxlaOy8nNYWfaThsSBzex9dBWth52LYe2\nknIkJb/ba56aETVpGNOQBtUa0CC6AQkxCXa90BIepmeCvkiDQDkjK8vOqfzJJ/DNN7B7t93eqlVB\nMHTuDFWrOlunKlZObg670ncVBITrMeVICtuPbCflcAoHMw+e8rmaETXzQ6F+dH3qRdejblRd6kXX\ny1/iI+N1alEv0yBQzjMG1qyxgTBnDvz4Ixw/btsRunQpOFto1Qq0W6TfOJp1lB1pO0g5bMMhb8kL\ni51pO4vcL5EnLCSMOlF18oOhblRd6kTVoU5UHWpH1i5Yj6qtZxgVRINA+Z6MDNuu8M03dklOttvr\n1IGOHQuW9u2Dbu6EQJOVk8We9D3sTNtZdEm3jzuO7GB3+m72H9tf7OdjqsQUCYbakbWpFVmLWpG1\niI+Iz1+vFVmLalWq6f0VJdAgUL4vJcWeKSxYAIsWwebNdnvlypCUVBAMHTrYsFAB50TOCfYe3cvu\n9N1Flj1H9xR5vvfoXg4fP1zsd1QOrVwkJGpG1KRmRM0i64WXGhE1gmbsJw0C5X9277bdUxcvtsuy\nZbbNAaBpUxsISUnQrp1dqutQDMHkePZx9mXsY+/RvflLakZq/vqeo3vYl7Evf8mbprQ41cOrU6Nq\nDWpE1CCuapxdr+pajyi6Hlc1jriqcVSrUs3v2jg0CJT/O37czrKWFwyLFxc0PgM0bgznnWdDIe+x\nQQNtb1CAvTxVOBjyltSjqezL2Mf+Y/s5cOwA+4/tZ3+GXS/prANsV9vq4dWJqxpHbNVYYsNj7Xp4\nbP7z2KqxVA+vnr/EhtvnMeExjpyFaBCowLR7N6xcCb/+WvD4xx+2YRrsHM3t2tnhtFu0gHPPtY81\ndKRPdXonck5wMPOgDYiM/flhcfDYwfztBzMPcvDYqes5JqfU746qHJUfEDFVYogJj7GPhdfDY055\nvUlsE6IqR5XreDQIVPBIS7O9kwqHw/r1dljtPPHxNhBOXvQMQlUAYwzpWekcyjxU6nIw0wbK4czD\nHD5+uMjjidwTxX73l/2/pEfzHuWqy90gCI4WExXYoqMLGpbz5ObaKTmTk4su06fbIbfzREVBs2bF\nLwkJOieDcouIEF0lmugq0STEJJT588YYMrMzTwmHw8cP075uew9UXJSeEajgYgzs3Vs0HDZtssuW\nLQWN02BDoHHjgmBo0sRO6dmwoV1q14YQ/2o8VMFFzwiUKo6I/QNeuzZ07Vr0tZwc2LGjIBg2bbJd\nWjdtgqVL7dAZhVWubM8a8oKhcEjUrw/16kFMjF56Uj7P60EgIgnAVKA2YIBJxpjXvF2HUqcIDS34\nQ37ppae+fugQbNtWsGzdWrA+fz7s3GkvSRUWEWEDIS8Y8h7z1uvWtaEUVb7GQKUqghNnBNnA34wx\nK0QkGlguInONMesdqEUp91WvbpfExOJfP3HCnlFs22Yfd+4s+rh0qX08fvzUz0ZE2Jvm8s5WCq/n\nLbVqQc2atgY9y1AVyOtBYIzZBexyraeJSDJQH9AgUP6tUiXbptC4ccnvMcY2VucFxK5ddl6H3bvt\n4549sHEjLFxoh/YuTliYDYSaNW1vqLwl73nNmhAXZ7vM5j1GRWl4qBI52kYgIo2B84Clxbw2AhgB\n0LBhQ6/WpZTHiNg/znFx0KZN6e89cQJSUwsCIjXVLvv2FV1fudKuHzx1VNB8lSoV7LdwQMTG2jOM\n2NiS13WyoYDnWK8hEYkCFgDPGGNmlfZe7TWklBuys2H/fhsOBw7YZf/+kh/377ftHkePlv694eE2\nFGJiii7FbctboqOhWjW7REfbS196RuJ1Pt1rSEQqATOBD04XAkopN4WFFbQnlEVWlp2X+uBBuxw6\ndOr6oUP2PYcP2/WUlILnGRmn30dISNFgyHvMW6KiSl+PioLIyILHiAjtuluBnOg1JMA7QLIx5t/e\n3r9S6iSVKxe0M5THiRNw5EhBSKSl2edHjhRdL7ykpdn3bt9u19PT7WN29un3lyci4tSAiIqy2yMj\nC5bSnletap+fvISHB1XQOHFG0AkYBKwRkZWubf8wxvzPgVqUUmeqUiXb3nCm4zkZY3tU5YVC3mNa\nmr18lZ5e8Fh4/eRte/fax4wM+3j0qL1HpKzyQqJq1VPXi3t+8hIeXvp6eHjRpXJlxy6fOdFraCGg\nFwuVUkWJFPxRrFmz4r7XGHv5q3AwHD1qx6LKyCh5yXs9772Fl8OHT92WkVG2M5rSjr/wMnGind7V\ng/TOYqVUYBOxM95VqWJ7QXlSdjZkZtpgyHssaT0z072lWjXP1owGgVJKVZywMNtO4Wd3igdPa4hS\nSqliaRAopVSQ0yBQSqkgp0GglFJBToNAKaWCnAaBUkoFOQ0CpZQKchoESikV5Pxi8noRSQW2lvPj\nNYESZvjwW4F2TIF2PBB4xxRoxwOBd0zFHU8jY8xpRxP0iyA4EyKyzJ3xuP1JoB1ToB0PBN4xBdrx\nQOAd05kcj14aUkqpIKdBoJRSQS4YgmCS0wV4QKAdU6AdDwTeMQXa8UDgHVO5jyfg2wiUUkqVLhjO\nCJRSSpUioINARK4Wkd9F5A8Redjpes6UiPwpImtEZKWILHO6nvIQkckisldE1hbaFicic0Vko+vR\nw7OHVJwSjudJEdnh+p1WikgPJ2ssKxFJEJHvRGS9iKwTkftc2/3ydyrlePz2dxKRcBH5WURWuY7p\nKdf2JiKy1PU3b5qIVHbr+wL10pCIhAIbgCuA7cAvQD9jzHpHCzsDIvInkGSM8du+zyLSBUgHphpj\nWru2vQgcMMY87wrsWGPMQ07W6a4SjudJIN0Y85KTtZWXiNQF6hpjVohINLAcuB4Ygh/+TqUcz834\n6e8kIgJEGmPSRaQSsBC4D3gAmGWM+VhEJgCrjDFvnu77AvmM4C/AH8aYzcaYLOBjoJfDNQU9Y8wP\nwIGTNvcCprjWp2D/J/ULJRyPXzPG7DLGrHCtpwHJQH389Hcq5Xj8lrHSXU8ruRYDXAbMcG13+zcK\n5CCoD6QUer4dP//xsT/0HBFZLiIjnC6mAtU2xuxyre8GajtZTAW5W0RWuy4d+cUllOKISGPgPGAp\nAfA7nXQ84Me/k4iEishKYC8wF9gEHDLGZLve4vbfvEAOgkB0sTGmPdAduMt1WSKgGHut0t+vV74J\nNAPaAbuAl50tp3xEJAqYCdxvjDlS+DV//J2KOR6//p2MMTnGmHZAA+wVkHPL+12BHAQ7gIRCzxu4\ntvktY8wO1+Ne4L/YHz8Q7HFdx827nrvX4XrOiDFmj+t/0lzgLfzwd3Jdd54JfGCMmeXa7Le/U3HH\nEwi/E4Ax5hDwHdABqC4iYa6X3P6bF8hB8AvQ3NWKXhm4Bfjc4ZrKTUQiXQ1diEgkcCWwtvRP+Y3P\ngcGu9cHAZw7Wcsby/li69MbPfidXQ+Q7QLIx5t+FXvLL36mk4/Hn30lE4kWkumu9KrZTTDI2EPq4\n3ub2bxSwvYYAXN3BXgVCgcnGmGccLqncRKQp9iwAIAz40B+PR0Q+ArpiR0rcAzwBfApMBxpiR5m9\n2RjjFw2wJRxPV+zlBgP8CYwsdG3d54nIxcCPwBog17X5H9jr6n73O5VyPP3w099JRBKxjcGh2H/Q\nTzfGPO36O/ExEAf8Cgw0xhw/7fcFchAopZQ6vUC+NKSUUsoNGgRKKRXkNAiUUirIaRAopVSQ0yBQ\nSqkgp0GglIeJSFcRme10HUqVRINAKaWCnAaBUi4iMtA1xvtKEZnoGtQrXURecY35Pl9E4l3vbSci\nP7kGLPtv3oBlInKWiMxzjRO/QkSaub4+SkRmiMhvIvKB625XpXyCBoFSgIi0APoCnVwDeeUAA4BI\nYJkxphWwAHvnMMBU4CFjTCL2jtW87R8A440xbYGO2MHMwI54eT/QEmgKdPL4QSnlprDTv0WpoNAN\nOB/4xfWP9arYQdVygWmu9/wHmCUiMUB1Y8wC1/YpwCeusaDqG2P+C2CMyQRwfd/PxpjtrucrgcbY\nyUSUcpwGgVKWAFOMMY8U2Sjy2EnvK++YLIXHe8lB/99TPkQvDSllzQf6iEgtyJ+ftxH2/5G80Rz7\nAwuNMYeBgyLS2bV9ELDANfvVdhG53vUdVUQkwqtHoVQ56L9KlAKMMetFZAx2BrgQ4ARwF3AU+Ivr\ntb3YdgSwQ/xOcP2h3wwMdW0fBEwUkadd33GTFw9DqXLR0UeVKoWIpBtjopyuQylP0ktDSikV5PSM\nQCmlgpyeESilVJDTIFBKqSCnQaCUUkFOg0AppYKcBoFSSgU5DQKllApy/w+XnEZIVA4ifQAAAABJ\nRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f2022b2de80>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(range(30), ld5, 'r')\n",
    "plt.plot(range(30), ld10, 'g')\n",
    "plt.plot(range(30), ld15, 'b')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('mse')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "----------------mf-----------------------\n",
    "----------------dnn----------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "userf = open('users.csv','r')\n",
    "userInfo = []\n",
    "userf.readline()\n",
    "for line in userf:\n",
    "    end = line.find('\\n')\n",
    "    line = line[:end]\n",
    "    userInfo.append(line.split('::'))\n",
    "userInfo = np.array(userInfo)\n",
    "userInfo_id = []\n",
    "userInfo_gender = []\n",
    "userInfo_age = []\n",
    "userInfo_occupation = []\n",
    "userInfo_zipcode = []\n",
    "zipcode_taglist = []\n",
    "for i in range(userInfo.shape[0]):\n",
    "    userInfo_id.append(userInfo[i,0])\n",
    "    if userInfo[i, 1] == 'F': \n",
    "        userInfo_gender.append('0')\n",
    "    else :\n",
    "        userInfo_gender.append('1')\n",
    "    userInfo_age.append(userInfo[i,2])\n",
    "    userInfo_occupation.append(userInfo[i,3])\n",
    "    userInfo_zipcode.append(userInfo[i,4])\n",
    "    if userInfo[i,4] not in zipcode_taglist:\n",
    "        zipcode_taglist.append(userInfo[i,4])\n",
    "userInfo_id = np.array(userInfo_id).astype(int)\n",
    "userInfo_gender = np.array(userInfo_gender).astype(float)\n",
    "userInfo_age = np.array(userInfo_age).astype(float)\n",
    "userInfo_occupation = np.array(userInfo_occupation).astype(int)\n",
    "userInfo_zipcode = np.array(userInfo_zipcode)\n",
    "\n",
    "### make user_id dict ###\n",
    "userIdMapIdx = dict.fromkeys(userInfo_id)\n",
    "for i in range(userInfo.shape[0]):\n",
    "    userIdMapIdx[userInfo_id[i]] = i\n",
    "### make zipcode dict ###\n",
    "zipcodeMapInt = dict.fromkeys(userInfo_id)\n",
    "for i in range(len(zipcode_taglist)):\n",
    "    zipcodeMapInt[zipcode_taglist[i]] = i\n",
    "    \n",
    "### create zipcode idx ###\n",
    "zipcode_index = np.zeros(userInfo.shape[0])\n",
    "for i in range(userInfo.shape[0]):\n",
    "    zipcode_index[i] = zipcodeMapInt[userInfo_zipcode[i]]\n",
    "zipcode_index = zipcode_index.astype(int)\n",
    "\n",
    "x_train_userInfo = []\n",
    "for i in range(numX):\n",
    "    temp = []\n",
    "    ti = userIdMapIdx[x_train_user[i]]\n",
    "    temp.append(userInfo_gender[ti])\n",
    "    temp.append(userInfo_age[ti])\n",
    "    temp.append(userInfo_occupation[ti])\n",
    "    temp.append(zipcode_index[ti])\n",
    "    x_train_userInfo.append(temp)\n",
    "x_train_userInfo = np.array(x_train_userInfo)\n",
    "x_train_userInfo = x_train_userInfo.astype(float)\n",
    "age = x_train_userInfo[:,1]\n",
    "nor_age = (age - np.mean(age))/np.std(age)\n",
    "x_train_userInfo[:,1] = nor_age \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:1: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "movief = pd.read_csv('movies.csv', sep='::', header = 0)\n",
    "#print(movieInfo['Genres'][3])\n",
    "#print(moviedata)\n",
    "#print(movieInfo.shape)\n",
    "Genres_taglist = []\n",
    "movieInfo = []\n",
    "movieInfoId = []\n",
    "numMovie = movief.shape[0]\n",
    "for i in range(numMovie):\n",
    "    movieInfo.append(movief['Genres'][i].split('|'))\n",
    "    movieInfoId.append(movief['movieID'][i])\n",
    "for i in range(numMovie):\n",
    "    for j in range(len(movieInfo[i])):\n",
    "        if movieInfo[i][j] not in Genres_taglist:\n",
    "            Genres_taglist.append(movieInfo[i][j])\n",
    "            \n",
    "#print(movieInfo)\n",
    "#print(Genres_taglist)\n",
    "Genres_len = len(Genres_taglist)\n",
    "Genres_vec = []\n",
    "### make movie id dict ###\n",
    "movieIdMapIdx = dict.fromkeys(movieInfoId)\n",
    "for i in range(numMovie):\n",
    "    movieIdMapIdx[movieInfoId[i]] = i\n",
    "for i in range(numMovie):\n",
    "    temp = []\n",
    "    for j in range(Genres_len):\n",
    "        if Genres_taglist[j] not in movieInfo[i]:\n",
    "            temp.append(0)\n",
    "        else:\n",
    "            temp.append(1)\n",
    "    Genres_vec.append(temp)\n",
    "Genres_vec = np.array(Genres_vec)\n",
    "\n",
    "x_train_movie = np.zeros((numX, Genres_len))\n",
    "for i in range(numX):\n",
    "    x_train_movie[i] = Genres_vec[  movieIdMapIdx[  x_train_item[i]  ]  ]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dnnmodel(n_users, n_items, n_occ, n_zip, latent_dim = 10):\n",
    "    user_input = Input(shape=[1])\n",
    "    item_input = Input(shape=[1])\n",
    "    user_occ = Input(shape=[1])\n",
    "    user_zip = Input(shape=[1])\n",
    "    user_Info = Input(shape=[2])\n",
    "    item_Info = Input(shape=[18])\n",
    "    user_vec = Embedding(n_users, latent_dim, embeddings_initializer='random_normal')(user_input)\n",
    "    user_vec = Flatten()(user_vec)\n",
    "    item_vec = Embedding(n_items, latent_dim, embeddings_initializer='random_normal')(item_input)\n",
    "    item_vec = Flatten()(item_vec)\n",
    "    #user_bias = Embedding(n_users, 1, embeddings_initializer='zeros')(user_input)\n",
    "    #user_bias = Flatten()(user_bias)\n",
    "    #item_bias = Embedding(n_items, 1, embeddings_initializer='zeros')(item_input)\n",
    "    #item_bias = Flatten()(item_bias)\n",
    "    \n",
    "    user_occ_vec = Embedding(n_occ, 20, embeddings_initializer='random_normal')(user_occ)\n",
    "    user_occ_vec = Flatten()(user_occ_vec)\n",
    "    user_zip_vec = Embedding(n_zip, 20, embeddings_initializer='random_normal')(user_zip)\n",
    "    user_zip_vec = Flatten()(user_zip_vec)\n",
    "    \n",
    "    merge_vec = Concatenate()([user_vec, item_vec, user_occ_vec, user_zip_vec, user_Info, item_Info])\n",
    "    hidden = Dense(500, activation='relu')(merge_vec)\n",
    "    hidden = Dropout(0.1)(hidden)\n",
    "    hidden = Dense(250, activation='relu')(hidden)\n",
    "    hidden = Dropout(0.1)(hidden)\n",
    "    hidden = Dense(125, activation='relu')(hidden)\n",
    "    output = Dense(1)(hidden)\n",
    "    model = keras.models.Model([user_input, item_input, user_occ, user_zip, user_Info, item_Info], output)\n",
    "    model.compile(loss='mse', optimizer='adam')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ncbs = [EarlyStopping('val_loss', patience=10), ModelCheckpoint('dnn3best.h5', save_best_only=True)]\\ndnn_model.fit([x_train_user[numV:], x_train_item[numV:], x_train_userInfo[numV:,2], \\n               x_train_userInfo[numV:,3], x_train_userInfo[numV:,:2],x_train_movie[numV:]]\\n              , y_train[numV:], \\n              batch_size = 128, \\n              epochs = 1000,\\n              validation_data = ( [x_train_user[:numV], x_train_item[:numV], x_train_userInfo[:numV,2], \\n                                 x_train_userInfo[:numV,3], x_train_userInfo[:numV,:2],x_train_movie[:numV]]\\n                                 , y_train[:numV])  \\n             ,callbacks=cbs)\\n\""
      ]
     },
     "execution_count": 431,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_len = 6040\n",
    "item_len = 3688\n",
    "latent_dim = 10\n",
    "n_occ = 21\n",
    "n_zip = 3439\n",
    "dnn_model = get_dnnmodel(user_len, item_len,  n_occ, n_zip, latent_dim)\n",
    "numX = 899873\n",
    "numV = 100000\n",
    "'''\n",
    "cbs = [EarlyStopping('val_loss', patience=10), ModelCheckpoint('dnn3best.h5', save_best_only=True)]\n",
    "dnn_model.fit([x_train_user[numV:], x_train_item[numV:], x_train_userInfo[numV:,2], \n",
    "               x_train_userInfo[numV:,3], x_train_userInfo[numV:,:2],x_train_movie[numV:]]\n",
    "              , y_train[numV:], \n",
    "              batch_size = 128, \n",
    "              epochs = 1000,\n",
    "              validation_data = ( [x_train_user[:numV], x_train_item[:numV], x_train_userInfo[:numV,2], \n",
    "                                 x_train_userInfo[:numV,3], x_train_userInfo[:numV,:2],x_train_movie[:numV]]\n",
    "                                 , y_train[:numV])  \n",
    "             ,callbacks=cbs)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 799873 samples, validate on 100000 samples\n",
      "Epoch 1/1\n",
      "799873/799873 [==============================] - 33s - loss: 165.9841 - val_loss: 0.7666\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f202dde1b70>"
      ]
     },
     "execution_count": 327,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dnn_model.fit([x_train_user[numV:], x_train_item[numV:], x_train_userInfo[numV:,2], \n",
    "               x_train_userInfo[numV:,3], x_train_userInfo[numV:,:2],x_train_movie[numV:]]\n",
    "              , y_train[numV:], \n",
    "              batch_size = 128, \n",
    "              epochs = 1,\n",
    "              validation_data = ( [x_train_user[:numV], x_train_item[:numV], x_train_userInfo[:numV,2], \n",
    "                                 x_train_userInfo[:numV,3], x_train_userInfo[:numV,:2],x_train_movie[:numV]]\n",
    "                                 , y_train[:numV]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0.         -2.4419657  10.          0.       ]\n"
     ]
    }
   ],
   "source": [
    "#print(np.min(x_train_userInfo[:,1]))\n",
    "print(x_test_userInfo[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 99616/100000 [============================>.] - ETA: 0s0.75117012321\n"
     ]
    }
   ],
   "source": [
    "dnn_model.load_weights('strongbest.h5')\n",
    "score = dnn_model.evaluate([x_train_user[:numV], x_train_item[:numV], x_train_userInfo[:numV,2], \n",
    "                                 x_train_userInfo[:numV,3], x_train_userInfo[:numV,:2],x_train_movie[:numV]]\n",
    "                                 , y_train[:numV])\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [],
   "source": [
    "dnn_model.load_weights('strongbest.h5')\n",
    "testf = open('test.csv','r')\n",
    "testf.readline()\n",
    "x_test = []\n",
    "for line in testf:\n",
    "    x_test.append(line.split(','))\n",
    "x_test = np.array(x_test)\n",
    "x_test_user = x_test[:, 1].astype(int)\n",
    "x_test_item = x_test[:, 2].astype(int)\n",
    "\n",
    "numT = 100336\n",
    "x_test_userInfo = []\n",
    "for i in range(numT):\n",
    "    temp = []\n",
    "    ti = userIdMapIdx[x_test_user[i]]\n",
    "    temp.append(userInfo_gender[ti])\n",
    "    temp.append(userInfo_age[ti])\n",
    "    temp.append(userInfo_occupation[ti])\n",
    "    temp.append(zipcode_index[ti])\n",
    "    x_test_userInfo.append(temp)\n",
    "x_test_userInfo = np.array(x_test_userInfo)\n",
    "x_test_userInfo = x_test_userInfo.astype(float)\n",
    "age = x_test_userInfo[:,1]\n",
    "nor_age = (age - np.mean(age))/np.std(age)\n",
    "x_test_userInfo[:,1] = nor_age \n",
    "\n",
    "\n",
    "numT = 100336\n",
    "Genres_len = 18\n",
    "x_test_movie = np.zeros((numT, Genres_len))\n",
    "for i in range(numT):\n",
    "    x_test_movie[i] = Genres_vec[  movieIdMapIdx[  x_test_item[i]  ]  ]\n",
    "    \n",
    "res = dnn_model.predict([x_test_user, x_test_item, x_test_userInfo[:,2], \n",
    "                    x_test_userInfo[:,3], x_test_userInfo[:,:2],x_test_movie])\n",
    "with open('predict2.csv', 'w') as f:\n",
    "    f.write('TestDataID,Rating\\n')\n",
    "    for i, v in  enumerate(res):\n",
    "        f.write('%d,%f\\n' %(i+1, v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_obj(obj, name ):\n",
    "    with open(name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_obj(name ):\n",
    "    with open(name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_obj(userIdMapIdx, 'userIdMapIdx')\n",
    "save_obj(userInfo_gender, 'userInfo_gender') \n",
    "save_obj(userInfo_age, 'userInfo_age') \n",
    "save_obj(userInfo_occupation, 'userInfo_occupation') \n",
    "save_obj(zipcode_index, 'zipcode_index') \n",
    "save_obj(Genres_vec, 'Genres_vec') \n",
    "save_obj(movieIdMapIdx, 'movieIdMapIdx') \n",
    "#newdic = load_obj('userIdMapIdx') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Dimension 0 in both shapes must be equal, but are 3688 and 6040 for 'Assign_797' (op: 'Assign') with input shapes: [3688,10], [6040,10].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/common_shapes.py\u001b[0m in \u001b[0;36m_call_cpp_shape_fn_impl\u001b[0;34m(op, input_tensors_needed, input_tensors_as_shapes_needed, debug_python_shape_fn, require_shape_fn)\u001b[0m\n\u001b[1;32m    670\u001b[0m           \u001b[0mgraph_def_version\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode_def_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shapes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_tensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 671\u001b[0;31m           input_tensors_as_shapes, status)\n\u001b[0m\u001b[1;32m    672\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m                 \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36mraise_exception_on_not_ok_status\u001b[0;34m()\u001b[0m\n\u001b[1;32m    465\u001b[0m           \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpywrap_tensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m           pywrap_tensorflow.TF_GetCode(status))\n\u001b[0m\u001b[1;32m    467\u001b[0m   \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Dimension 0 in both shapes must be equal, but are 3688 and 6040 for 'Assign_797' (op: 'Assign') with input shapes: [3688,10], [6040,10].",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-430-62464695e4ad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m#emb = get_emb()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mtemp_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitem_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlatent_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mtemp_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'8743.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;31m#tmodel = TSNE(n_components=2, random_state=0)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m#np.set_printoptions(suppress=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/topology.py\u001b[0m in \u001b[0;36mload_weights\u001b[0;34m(self, filepath, by_name)\u001b[0m\n\u001b[1;32m   2498\u001b[0m             \u001b[0mload_weights_from_hdf5_group_by_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2499\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2500\u001b[0;31m             \u001b[0mload_weights_from_hdf5_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2501\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2502\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'close'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/topology.py\u001b[0m in \u001b[0;36mload_weights_from_hdf5_group\u001b[0;34m(f, layers)\u001b[0m\n\u001b[1;32m   2911\u001b[0m                              ' elements.')\n\u001b[1;32m   2912\u001b[0m         \u001b[0mweight_value_tuples\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msymbolic_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2913\u001b[0;31m     \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_set_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight_value_tuples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2914\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2915\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36mbatch_set_value\u001b[0;34m(tuples)\u001b[0m\n\u001b[1;32m   2020\u001b[0m                 assign_placeholder = tf.placeholder(tf_dtype,\n\u001b[1;32m   2021\u001b[0m                                                     shape=value.shape)\n\u001b[0;32m-> 2022\u001b[0;31m                 \u001b[0massign_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0massign_placeholder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2023\u001b[0m                 \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_assign_placeholder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0massign_placeholder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2024\u001b[0m                 \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_assign_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0massign_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/variables.py\u001b[0m in \u001b[0;36massign\u001b[0;34m(self, value, use_locking)\u001b[0m\n\u001b[1;32m    520\u001b[0m       \u001b[0mthe\u001b[0m \u001b[0massignment\u001b[0m \u001b[0mhas\u001b[0m \u001b[0mcompleted\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m     \"\"\"\n\u001b[0;32m--> 522\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mstate_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_locking\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_locking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    523\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0massign_add\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_locking\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gen_state_ops.py\u001b[0m in \u001b[0;36massign\u001b[0;34m(ref, value, validate_shape, use_locking, name)\u001b[0m\n\u001b[1;32m     45\u001b[0m   result = _op_def_lib.apply_op(\"Assign\", ref=ref, value=value,\n\u001b[1;32m     46\u001b[0m                                 \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m                                 use_locking=use_locking, name=name)\n\u001b[0m\u001b[1;32m     48\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36mapply_op\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    761\u001b[0m         op = g.create_op(op_type_name, inputs, output_types, name=scope,\n\u001b[1;32m    762\u001b[0m                          \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattr_protos\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 763\u001b[0;31m                          op_def=op_def)\n\u001b[0m\u001b[1;32m    764\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0moutput_structure\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mcreate_op\u001b[0;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_shapes, compute_device)\u001b[0m\n\u001b[1;32m   2327\u001b[0m                     original_op=self._default_original_op, op_def=op_def)\n\u001b[1;32m   2328\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcompute_shapes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2329\u001b[0;31m       \u001b[0mset_shapes_for_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2330\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2331\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_record_op_seen_by_control_dependencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mset_shapes_for_outputs\u001b[0;34m(op)\u001b[0m\n\u001b[1;32m   1715\u001b[0m       \u001b[0mshape_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_call_cpp_shape_fn_and_require_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1716\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1717\u001b[0;31m   \u001b[0mshapes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshape_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1718\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mshapes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1719\u001b[0m     raise RuntimeError(\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mcall_with_requiring\u001b[0;34m(op)\u001b[0m\n\u001b[1;32m   1665\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1666\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_with_requiring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1667\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcall_cpp_shape_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequire_shape_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1668\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1669\u001b[0m   \u001b[0m_call_cpp_shape_fn_and_require_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_with_requiring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/common_shapes.py\u001b[0m in \u001b[0;36mcall_cpp_shape_fn\u001b[0;34m(op, input_tensors_needed, input_tensors_as_shapes_needed, debug_python_shape_fn, require_shape_fn)\u001b[0m\n\u001b[1;32m    608\u001b[0m     res = _call_cpp_shape_fn_impl(op, input_tensors_needed,\n\u001b[1;32m    609\u001b[0m                                   \u001b[0minput_tensors_as_shapes_needed\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 610\u001b[0;31m                                   debug_python_shape_fn, require_shape_fn)\n\u001b[0m\u001b[1;32m    611\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m       \u001b[0;31m# Handles the case where _call_cpp_shape_fn_impl calls unknown_shape(op).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/common_shapes.py\u001b[0m in \u001b[0;36m_call_cpp_shape_fn_impl\u001b[0;34m(op, input_tensors_needed, input_tensors_as_shapes_needed, debug_python_shape_fn, require_shape_fn)\u001b[0m\n\u001b[1;32m    674\u001b[0m       \u001b[0mmissing_shape_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mmissing_shape_fn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Dimension 0 in both shapes must be equal, but are 3688 and 6040 for 'Assign_797' (op: 'Assign') with input shapes: [3688,10], [6040,10]."
     ]
    }
   ],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "user_len = 6040\n",
    "item_len = 3688\n",
    "\n",
    "latent_dim = 10\n",
    "#def get_emb():\n",
    "    #movie_vec = Input(shape=[1])\n",
    "    #out = Embedding(3688, 10, embeddings_initializer='random_normal')(movie_vec)\n",
    "    #model = keras.models.Model([movie_vec], out)\n",
    "    #model.compile(loss='mse', optimizer='adam')\n",
    "    #return model\n",
    "#emb = get_emb()\n",
    "temp_model = get_model(user_len, item_len, latent_dim)\n",
    "temp_model.load_weights('8743.h5')\n",
    "#tmodel = TSNE(n_components=2, random_state=0)\n",
    "#np.set_printoptions(suppress=True)\n",
    "#wvector = tmodel.fit_transform(x)\n",
    "#print(wvector)\n",
    "#temp_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nuserIdMapIdx = load_obj('userIdMapIdx')\n",
    "nuserInfo_gender = load_obj('userInfo_gender') \n",
    "nuserInfo_age = load_obj('userInfo_age') \n",
    "nuserInfo_occupation = load_obj('userInfo_occupation') \n",
    "nzipcode_index = load_obj('zipcode_index') \n",
    "nGenres_vec = load_obj('Genres_vec') \n",
    "nmovieIdMapIdx = load_obj('movieIdMapIdx') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 0, 2: 1, 3: 2, 4: 3, 5: 4, 6: 5, 7: 6, 8: 7, 9: 8, 10: 9, 11: 10, 12: 11, 13: 12, 14: 13, 15: 14, 16: 15, 17: 16, 18: 17, 19: 18, 20: 19, 21: 20, 22: 21, 23: 22, 24: 23, 25: 24, 26: 25, 27: 26, 28: 27, 29: 28, 30: 29, 31: 30, 32: 31, 33: 32, 34: 33, 35: 34, 36: 35, 37: 36, 38: 37, 39: 38, 40: 39, 41: 40, 42: 41, 43: 42, 44: 43, 45: 44, 46: 45, 47: 46, 48: 47, 49: 48, 50: 49, 51: 50, 52: 51, 53: 52, 54: 53, 55: 54, 56: 55, 57: 56, 58: 57, 59: 58, 60: 59, 61: 60, 62: 61, 63: 62, 64: 63, 65: 64, 66: 65, 67: 66, 68: 67, 69: 68, 70: 69, 71: 70, 72: 71, 73: 72, 74: 73, 75: 74, 76: 75, 77: 76, 78: 77, 79: 78, 80: 79, 81: 80, 82: 81, 83: 82, 84: 83, 85: 84, 86: 85, 87: 86, 88: 87, 89: 88, 90: 89, 92: 90, 93: 91, 94: 92, 95: 93, 96: 94, 97: 95, 98: 96, 99: 97, 100: 98, 101: 99, 102: 100, 103: 101, 104: 102, 105: 103, 106: 104, 107: 105, 108: 106, 109: 107, 110: 108, 111: 109, 112: 110, 113: 111, 114: 112, 115: 113, 116: 114, 117: 115, 118: 116, 119: 117, 120: 118, 121: 119, 122: 120, 123: 121, 124: 122, 125: 123, 126: 124, 127: 125, 128: 126, 129: 127, 130: 128, 131: 129, 132: 130, 133: 131, 134: 132, 135: 133, 136: 134, 137: 135, 138: 136, 139: 137, 140: 138, 141: 139, 142: 140, 143: 141, 144: 142, 145: 143, 146: 144, 147: 145, 148: 146, 149: 147, 150: 148, 151: 149, 152: 150, 153: 151, 154: 152, 155: 153, 156: 154, 157: 155, 158: 156, 159: 157, 160: 158, 161: 159, 162: 160, 163: 161, 164: 162, 165: 163, 166: 164, 167: 165, 168: 166, 169: 167, 170: 168, 171: 169, 172: 170, 173: 171, 174: 172, 175: 173, 176: 174, 177: 175, 178: 176, 179: 177, 180: 178, 181: 179, 182: 180, 183: 181, 184: 182, 185: 183, 186: 184, 187: 185, 188: 186, 189: 187, 190: 188, 191: 189, 192: 190, 193: 191, 194: 192, 195: 193, 196: 194, 197: 195, 198: 196, 199: 197, 200: 198, 201: 199, 202: 200, 203: 201, 204: 202, 205: 203, 206: 204, 207: 205, 208: 206, 209: 207, 210: 208, 211: 209, 212: 210, 213: 211, 214: 212, 215: 213, 216: 214, 217: 215, 218: 216, 219: 217, 220: 218, 222: 219, 223: 220, 224: 221, 225: 222, 226: 223, 227: 224, 228: 225, 229: 226, 230: 227, 231: 228, 232: 229, 233: 230, 234: 231, 235: 232, 236: 233, 237: 234, 238: 235, 239: 236, 240: 237, 241: 238, 242: 239, 243: 240, 244: 241, 245: 242, 246: 243, 247: 244, 248: 245, 249: 246, 250: 247, 251: 248, 252: 249, 253: 250, 254: 251, 255: 252, 256: 253, 257: 254, 258: 255, 259: 256, 260: 257, 261: 258, 262: 259, 263: 260, 264: 261, 265: 262, 266: 263, 267: 264, 268: 265, 269: 266, 270: 267, 271: 268, 272: 269, 273: 270, 274: 271, 275: 272, 276: 273, 277: 274, 278: 275, 279: 276, 280: 277, 281: 278, 282: 279, 283: 280, 284: 281, 285: 282, 286: 283, 287: 284, 288: 285, 289: 286, 290: 287, 291: 288, 292: 289, 293: 290, 294: 291, 295: 292, 296: 293, 297: 294, 298: 295, 299: 296, 300: 297, 301: 298, 302: 299, 303: 300, 304: 301, 305: 302, 306: 303, 307: 304, 308: 305, 309: 306, 310: 307, 311: 308, 312: 309, 313: 310, 314: 311, 315: 312, 316: 313, 317: 314, 318: 315, 319: 316, 320: 317, 321: 318, 322: 319, 324: 320, 325: 321, 326: 322, 327: 323, 328: 324, 329: 325, 330: 326, 331: 327, 332: 328, 333: 329, 334: 330, 335: 331, 336: 332, 337: 333, 338: 334, 339: 335, 340: 336, 341: 337, 342: 338, 343: 339, 344: 340, 345: 341, 346: 342, 347: 343, 348: 344, 349: 345, 350: 346, 351: 347, 352: 348, 353: 349, 354: 350, 355: 351, 356: 352, 357: 353, 358: 354, 359: 355, 360: 356, 361: 357, 362: 358, 363: 359, 364: 360, 365: 361, 366: 362, 367: 363, 368: 364, 369: 365, 370: 366, 371: 367, 372: 368, 373: 369, 374: 370, 375: 371, 376: 372, 377: 373, 378: 374, 379: 375, 380: 376, 381: 377, 382: 378, 383: 379, 384: 380, 385: 381, 386: 382, 387: 383, 388: 384, 389: 385, 390: 386, 391: 387, 392: 388, 393: 389, 394: 390, 395: 391, 396: 392, 397: 393, 398: 394, 399: 395, 400: 396, 401: 397, 402: 398, 403: 399, 404: 400, 405: 401, 406: 402, 407: 403, 408: 404, 409: 405, 410: 406, 411: 407, 412: 408, 413: 409, 414: 410, 415: 411, 416: 412, 417: 413, 418: 414, 419: 415, 420: 416, 421: 417, 422: 418, 423: 419, 424: 420, 425: 421, 426: 422, 427: 423, 428: 424, 429: 425, 430: 426, 431: 427, 432: 428, 433: 429, 434: 430, 435: 431, 436: 432, 437: 433, 438: 434, 439: 435, 440: 436, 441: 437, 442: 438, 443: 439, 444: 440, 445: 441, 446: 442, 447: 443, 448: 444, 449: 445, 450: 446, 451: 447, 452: 448, 453: 449, 454: 450, 455: 451, 456: 452, 457: 453, 458: 454, 459: 455, 460: 456, 461: 457, 462: 458, 463: 459, 464: 460, 465: 461, 466: 462, 467: 463, 468: 464, 469: 465, 470: 466, 471: 467, 472: 468, 473: 469, 474: 470, 475: 471, 476: 472, 477: 473, 478: 474, 479: 475, 480: 476, 481: 477, 482: 478, 483: 479, 484: 480, 485: 481, 486: 482, 487: 483, 488: 484, 489: 485, 490: 486, 491: 487, 492: 488, 493: 489, 494: 490, 495: 491, 496: 492, 497: 493, 498: 494, 499: 495, 500: 496, 501: 497, 502: 498, 503: 499, 504: 500, 505: 501, 506: 502, 507: 503, 508: 504, 509: 505, 510: 506, 511: 507, 512: 508, 513: 509, 514: 510, 515: 511, 516: 512, 517: 513, 518: 514, 519: 515, 520: 516, 521: 517, 522: 518, 523: 519, 524: 520, 525: 521, 526: 522, 527: 523, 528: 524, 529: 525, 530: 526, 531: 527, 532: 528, 533: 529, 534: 530, 535: 531, 536: 532, 537: 533, 538: 534, 539: 535, 540: 536, 541: 537, 542: 538, 543: 539, 544: 540, 545: 541, 546: 542, 547: 543, 548: 544, 549: 545, 550: 546, 551: 547, 552: 548, 553: 549, 554: 550, 555: 551, 556: 552, 557: 553, 558: 554, 559: 555, 560: 556, 561: 557, 562: 558, 563: 559, 564: 560, 565: 561, 566: 562, 567: 563, 568: 564, 569: 565, 570: 566, 571: 567, 572: 568, 573: 569, 574: 570, 575: 571, 576: 572, 577: 573, 578: 574, 579: 575, 580: 576, 581: 577, 582: 578, 583: 579, 584: 580, 585: 581, 586: 582, 587: 583, 588: 584, 589: 585, 590: 586, 591: 587, 592: 588, 593: 589, 594: 590, 595: 591, 596: 592, 597: 593, 598: 594, 599: 595, 600: 596, 601: 597, 602: 598, 603: 599, 604: 600, 605: 601, 606: 602, 607: 603, 608: 604, 609: 605, 610: 606, 611: 607, 612: 608, 613: 609, 614: 610, 615: 611, 616: 612, 617: 613, 618: 614, 619: 615, 620: 616, 621: 617, 623: 618, 624: 619, 625: 620, 626: 621, 627: 622, 628: 623, 629: 624, 630: 625, 631: 626, 632: 627, 633: 628, 634: 629, 635: 630, 636: 631, 637: 632, 638: 633, 639: 634, 640: 635, 641: 636, 642: 637, 643: 638, 644: 639, 645: 640, 647: 641, 648: 642, 649: 643, 650: 644, 651: 645, 652: 646, 653: 647, 654: 648, 655: 649, 656: 650, 657: 651, 658: 652, 659: 653, 660: 654, 661: 655, 662: 656, 663: 657, 664: 658, 665: 659, 666: 660, 667: 661, 668: 662, 669: 663, 670: 664, 671: 665, 672: 666, 673: 667, 674: 668, 675: 669, 676: 670, 678: 671, 679: 672, 680: 673, 681: 674, 682: 675, 683: 676, 684: 677, 685: 678, 687: 679, 688: 680, 690: 681, 691: 682, 692: 683, 693: 684, 694: 685, 695: 686, 696: 687, 697: 688, 698: 689, 699: 690, 700: 691, 701: 692, 702: 693, 703: 694, 704: 695, 705: 696, 706: 697, 707: 698, 708: 699, 709: 700, 710: 701, 711: 702, 712: 703, 713: 704, 714: 705, 715: 706, 716: 707, 717: 708, 718: 709, 719: 710, 720: 711, 721: 712, 722: 713, 723: 714, 724: 715, 725: 716, 726: 717, 727: 718, 728: 719, 729: 720, 730: 721, 731: 722, 732: 723, 733: 724, 734: 725, 735: 726, 736: 727, 737: 728, 738: 729, 739: 730, 741: 731, 742: 732, 743: 733, 744: 734, 745: 735, 746: 736, 747: 737, 748: 738, 749: 739, 750: 740, 751: 741, 752: 742, 753: 743, 754: 744, 755: 745, 756: 746, 757: 747, 758: 748, 759: 749, 760: 750, 761: 751, 762: 752, 763: 753, 764: 754, 765: 755, 766: 756, 767: 757, 768: 758, 769: 759, 770: 760, 771: 761, 772: 762, 773: 763, 774: 764, 775: 765, 776: 766, 777: 767, 778: 768, 779: 769, 780: 770, 781: 771, 782: 772, 783: 773, 784: 774, 785: 775, 786: 776, 787: 777, 788: 778, 789: 779, 790: 780, 791: 781, 792: 782, 793: 783, 794: 784, 795: 785, 796: 786, 797: 787, 798: 788, 799: 789, 800: 790, 801: 791, 802: 792, 803: 793, 804: 794, 805: 795, 806: 796, 807: 797, 808: 798, 809: 799, 810: 800, 811: 801, 812: 802, 813: 803, 814: 804, 815: 805, 816: 806, 818: 807, 819: 808, 820: 809, 821: 810, 822: 811, 823: 812, 824: 813, 825: 814, 826: 815, 827: 816, 828: 817, 829: 818, 830: 819, 831: 820, 832: 821, 833: 822, 834: 823, 835: 824, 836: 825, 837: 826, 838: 827, 839: 828, 840: 829, 841: 830, 842: 831, 843: 832, 844: 833, 845: 834, 846: 835, 847: 836, 848: 837, 849: 838, 850: 839, 851: 840, 852: 841, 853: 842, 854: 843, 855: 844, 856: 845, 857: 846, 858: 847, 859: 848, 860: 849, 861: 850, 862: 851, 863: 852, 864: 853, 865: 854, 866: 855, 867: 856, 868: 857, 869: 858, 870: 859, 871: 860, 872: 861, 873: 862, 874: 863, 875: 864, 876: 865, 877: 866, 878: 867, 879: 868, 880: 869, 881: 870, 882: 871, 884: 872, 885: 873, 886: 874, 887: 875, 888: 876, 889: 877, 890: 878, 891: 879, 892: 880, 893: 881, 894: 882, 895: 883, 896: 884, 897: 885, 898: 886, 899: 887, 900: 888, 901: 889, 902: 890, 903: 891, 904: 892, 905: 893, 906: 894, 907: 895, 908: 896, 909: 897, 910: 898, 911: 899, 912: 900, 913: 901, 914: 902, 915: 903, 916: 904, 917: 905, 918: 906, 919: 907, 920: 908, 921: 909, 922: 910, 923: 911, 924: 912, 925: 913, 926: 914, 927: 915, 928: 916, 929: 917, 930: 918, 931: 919, 932: 920, 933: 921, 934: 922, 935: 923, 936: 924, 937: 925, 938: 926, 939: 927, 940: 928, 941: 929, 942: 930, 943: 931, 944: 932, 945: 933, 946: 934, 947: 935, 948: 936, 949: 937, 950: 938, 951: 939, 952: 940, 953: 941, 954: 942, 955: 943, 956: 944, 957: 945, 958: 946, 959: 947, 960: 948, 961: 949, 962: 950, 963: 951, 964: 952, 965: 953, 966: 954, 967: 955, 968: 956, 969: 957, 970: 958, 971: 959, 972: 960, 973: 961, 974: 962, 975: 963, 976: 964, 977: 965, 978: 966, 979: 967, 980: 968, 981: 969, 982: 970, 983: 971, 984: 972, 985: 973, 986: 974, 987: 975, 988: 976, 989: 977, 990: 978, 991: 979, 992: 980, 993: 981, 994: 982, 996: 983, 997: 984, 998: 985, 999: 986, 1000: 987, 1001: 988, 1002: 989, 1003: 990, 1004: 991, 1005: 992, 1006: 993, 1007: 994, 1008: 995, 1009: 996, 1010: 997, 1011: 998, 1012: 999, 1013: 1000, 1014: 1001, 1015: 1002, 1016: 1003, 1017: 1004, 1018: 1005, 1019: 1006, 1020: 1007, 1021: 1008, 1022: 1009, 1023: 1010, 1024: 1011, 1025: 1012, 1026: 1013, 1027: 1014, 1028: 1015, 1029: 1016, 1030: 1017, 1031: 1018, 1032: 1019, 1033: 1020, 1034: 1021, 1035: 1022, 1036: 1023, 1037: 1024, 1038: 1025, 1039: 1026, 1040: 1027, 1041: 1028, 1042: 1029, 1043: 1030, 1044: 1031, 1045: 1032, 1046: 1033, 1047: 1034, 1049: 1035, 1050: 1036, 1051: 1037, 1052: 1038, 1053: 1039, 1054: 1040, 1055: 1041, 1056: 1042, 1057: 1043, 1058: 1044, 1059: 1045, 1060: 1046, 1061: 1047, 1062: 1048, 1063: 1049, 1064: 1050, 1065: 1051, 1066: 1052, 1067: 1053, 1068: 1054, 1069: 1055, 1070: 1056, 1071: 1057, 1073: 1058, 1075: 1059, 1076: 1060, 1077: 1061, 1078: 1062, 1079: 1063, 1080: 1064, 1081: 1065, 1082: 1066, 1083: 1067, 1084: 1068, 1085: 1069, 1086: 1070, 1087: 1071, 1088: 1072, 1089: 1073, 1090: 1074, 1091: 1075, 1092: 1076, 1093: 1077, 1094: 1078, 1095: 1079, 1096: 1080, 1097: 1081, 1098: 1082, 1099: 1083, 1100: 1084, 1101: 1085, 1102: 1086, 1103: 1087, 1104: 1088, 1105: 1089, 1106: 1090, 1107: 1091, 1108: 1092, 1109: 1093, 1110: 1094, 1111: 1095, 1112: 1096, 1113: 1097, 1114: 1098, 1115: 1099, 1116: 1100, 1117: 1101, 1118: 1102, 1119: 1103, 1120: 1104, 1121: 1105, 1122: 1106, 1123: 1107, 1124: 1108, 1125: 1109, 1126: 1110, 1127: 1111, 1128: 1112, 1129: 1113, 1130: 1114, 1131: 1115, 1132: 1116, 1133: 1117, 1134: 1118, 1135: 1119, 1136: 1120, 1137: 1121, 1138: 1122, 1139: 1123, 1140: 1124, 1141: 1125, 1142: 1126, 1143: 1127, 1144: 1128, 1145: 1129, 1146: 1130, 1147: 1131, 1148: 1132, 1149: 1133, 1150: 1134, 1151: 1135, 1152: 1136, 1153: 1137, 1154: 1138, 1155: 1139, 1156: 1140, 1157: 1141, 1158: 1142, 1159: 1143, 1160: 1144, 1161: 1145, 1162: 1146, 1163: 1147, 1164: 1148, 1165: 1149, 1166: 1150, 1167: 1151, 1168: 1152, 1169: 1153, 1170: 1154, 1171: 1155, 1172: 1156, 1173: 1157, 1174: 1158, 1175: 1159, 1176: 1160, 1177: 1161, 1178: 1162, 1179: 1163, 1180: 1164, 1181: 1165, 1183: 1166, 1184: 1167, 1185: 1168, 1186: 1169, 1187: 1170, 1188: 1171, 1189: 1172, 1190: 1173, 1191: 1174, 1192: 1175, 1193: 1176, 1194: 1177, 1196: 1178, 1197: 1179, 1198: 1180, 1199: 1181, 1200: 1182, 1201: 1183, 1202: 1184, 1203: 1185, 1204: 1186, 1205: 1187, 1206: 1188, 1207: 1189, 1208: 1190, 1209: 1191, 1210: 1192, 1211: 1193, 1212: 1194, 1213: 1195, 1214: 1196, 1215: 1197, 1216: 1198, 1217: 1199, 1218: 1200, 1219: 1201, 1220: 1202, 1221: 1203, 1222: 1204, 1223: 1205, 1224: 1206, 1225: 1207, 1226: 1208, 1227: 1209, 1228: 1210, 1230: 1211, 1231: 1212, 1232: 1213, 1233: 1214, 1234: 1215, 1235: 1216, 1236: 1217, 1237: 1218, 1238: 1219, 1240: 1220, 1241: 1221, 1242: 1222, 1243: 1223, 1244: 1224, 1245: 1225, 1246: 1226, 1247: 1227, 1248: 1228, 1249: 1229, 1250: 1230, 1251: 1231, 1252: 1232, 1253: 1233, 1254: 1234, 1255: 1235, 1256: 1236, 1257: 1237, 1258: 1238, 1259: 1239, 1260: 1240, 1261: 1241, 1262: 1242, 1263: 1243, 1264: 1244, 1265: 1245, 1266: 1246, 1267: 1247, 1268: 1248, 1269: 1249, 1270: 1250, 1271: 1251, 1272: 1252, 1273: 1253, 1274: 1254, 1275: 1255, 1276: 1256, 1277: 1257, 1278: 1258, 1279: 1259, 1280: 1260, 1281: 1261, 1282: 1262, 1283: 1263, 1284: 1264, 1285: 1265, 1286: 1266, 1287: 1267, 1288: 1268, 1289: 1269, 1290: 1270, 1291: 1271, 1292: 1272, 1293: 1273, 1294: 1274, 1295: 1275, 1296: 1276, 1297: 1277, 1298: 1278, 1299: 1279, 1300: 1280, 1301: 1281, 1302: 1282, 1303: 1283, 1304: 1284, 1305: 1285, 1306: 1286, 1307: 1287, 1308: 1288, 1309: 1289, 1310: 1290, 1311: 1291, 1312: 1292, 1313: 1293, 1314: 1294, 1315: 1295, 1316: 1296, 1317: 1297, 1318: 1298, 1319: 1299, 1320: 1300, 1321: 1301, 1322: 1302, 1323: 1303, 1324: 1304, 1325: 1305, 1326: 1306, 1327: 1307, 1328: 1308, 1329: 1309, 1330: 1310, 1331: 1311, 1332: 1312, 1333: 1313, 1334: 1314, 1335: 1315, 1336: 1316, 1337: 1317, 1339: 1318, 1340: 1319, 1341: 1320, 1342: 1321, 1343: 1322, 1344: 1323, 1345: 1324, 1346: 1325, 1347: 1326, 1348: 1327, 1349: 1328, 1350: 1329, 1351: 1330, 1352: 1331, 1353: 1332, 1354: 1333, 1355: 1334, 1356: 1335, 1357: 1336, 1358: 1337, 1359: 1338, 1360: 1339, 1361: 1340, 1362: 1341, 1363: 1342, 1364: 1343, 1365: 1344, 1366: 1345, 1367: 1346, 1368: 1347, 1369: 1348, 1370: 1349, 1371: 1350, 1372: 1351, 1373: 1352, 1374: 1353, 1375: 1354, 1376: 1355, 1377: 1356, 1378: 1357, 1379: 1358, 1380: 1359, 1381: 1360, 1382: 1361, 1383: 1362, 1384: 1363, 1385: 1364, 1386: 1365, 1387: 1366, 1388: 1367, 1389: 1368, 1390: 1369, 1391: 1370, 1392: 1371, 1393: 1372, 1394: 1373, 1395: 1374, 1396: 1375, 1397: 1376, 1398: 1377, 1399: 1378, 1400: 1379, 1401: 1380, 1404: 1381, 1405: 1382, 1406: 1383, 1407: 1384, 1408: 1385, 1409: 1386, 1410: 1387, 1411: 1388, 1412: 1389, 1413: 1390, 1414: 1391, 1415: 1392, 1416: 1393, 1417: 1394, 1419: 1395, 1420: 1396, 1421: 1397, 1422: 1398, 1423: 1399, 1424: 1400, 1425: 1401, 1426: 1402, 1427: 1403, 1428: 1404, 1429: 1405, 1430: 1406, 1431: 1407, 1432: 1408, 1433: 1409, 1434: 1410, 1436: 1411, 1437: 1412, 1438: 1413, 1439: 1414, 1440: 1415, 1441: 1416, 1442: 1417, 1443: 1418, 1444: 1419, 1445: 1420, 1446: 1421, 1447: 1422, 1448: 1423, 1449: 1424, 1450: 1425, 1453: 1426, 1454: 1427, 1455: 1428, 1456: 1429, 1457: 1430, 1458: 1431, 1459: 1432, 1460: 1433, 1461: 1434, 1462: 1435, 1463: 1436, 1464: 1437, 1465: 1438, 1466: 1439, 1467: 1440, 1468: 1441, 1470: 1442, 1471: 1443, 1472: 1444, 1473: 1445, 1474: 1446, 1475: 1447, 1476: 1448, 1477: 1449, 1479: 1450, 1480: 1451, 1482: 1452, 1483: 1453, 1484: 1454, 1485: 1455, 1486: 1456, 1487: 1457, 1488: 1458, 1489: 1459, 1490: 1460, 1493: 1461, 1494: 1462, 1495: 1463, 1496: 1464, 1497: 1465, 1498: 1466, 1499: 1467, 1500: 1468, 1501: 1469, 1502: 1470, 1503: 1471, 1504: 1472, 1507: 1473, 1508: 1474, 1509: 1475, 1510: 1476, 1511: 1477, 1513: 1478, 1514: 1479, 1515: 1480, 1516: 1481, 1517: 1482, 1518: 1483, 1519: 1484, 1520: 1485, 1522: 1486, 1523: 1487, 1524: 1488, 1525: 1489, 1526: 1490, 1527: 1491, 1528: 1492, 1529: 1493, 1531: 1494, 1532: 1495, 1533: 1496, 1534: 1497, 1535: 1498, 1537: 1499, 1538: 1500, 1539: 1501, 1541: 1502, 1542: 1503, 1543: 1504, 1544: 1505, 1545: 1506, 1546: 1507, 1547: 1508, 1548: 1509, 1549: 1510, 1550: 1511, 1551: 1512, 1552: 1513, 1553: 1514, 1554: 1515, 1555: 1516, 1556: 1517, 1557: 1518, 1558: 1519, 1559: 1520, 1561: 1521, 1562: 1522, 1563: 1523, 1564: 1524, 1565: 1525, 1566: 1526, 1567: 1527, 1568: 1528, 1569: 1529, 1570: 1530, 1571: 1531, 1572: 1532, 1573: 1533, 1574: 1534, 1575: 1535, 1577: 1536, 1578: 1537, 1579: 1538, 1580: 1539, 1581: 1540, 1582: 1541, 1583: 1542, 1584: 1543, 1585: 1544, 1586: 1545, 1587: 1546, 1588: 1547, 1589: 1548, 1590: 1549, 1591: 1550, 1592: 1551, 1593: 1552, 1594: 1553, 1595: 1554, 1596: 1555, 1597: 1556, 1598: 1557, 1599: 1558, 1600: 1559, 1601: 1560, 1602: 1561, 1603: 1562, 1604: 1563, 1605: 1564, 1606: 1565, 1608: 1566, 1609: 1567, 1610: 1568, 1611: 1569, 1612: 1570, 1613: 1571, 1614: 1572, 1615: 1573, 1616: 1574, 1617: 1575, 1619: 1576, 1620: 1577, 1621: 1578, 1622: 1579, 1623: 1580, 1624: 1581, 1625: 1582, 1626: 1583, 1627: 1584, 1628: 1585, 1629: 1586, 1630: 1587, 1631: 1588, 1632: 1589, 1633: 1590, 1635: 1591, 1636: 1592, 1639: 1593, 1640: 1594, 1641: 1595, 1642: 1596, 1643: 1597, 1644: 1598, 1645: 1599, 1646: 1600, 1647: 1601, 1648: 1602, 1649: 1603, 1650: 1604, 1651: 1605, 1652: 1606, 1653: 1607, 1654: 1608, 1655: 1609, 1656: 1610, 1657: 1611, 1658: 1612, 1659: 1613, 1660: 1614, 1661: 1615, 1662: 1616, 1663: 1617, 1664: 1618, 1665: 1619, 1666: 1620, 1667: 1621, 1668: 1622, 1669: 1623, 1670: 1624, 1671: 1625, 1672: 1626, 1673: 1627, 1674: 1628, 1675: 1629, 1676: 1630, 1677: 1631, 1678: 1632, 1679: 1633, 1680: 1634, 1681: 1635, 1682: 1636, 1683: 1637, 1684: 1638, 1685: 1639, 1686: 1640, 1687: 1641, 1688: 1642, 1689: 1643, 1690: 1644, 1692: 1645, 1693: 1646, 1694: 1647, 1695: 1648, 1696: 1649, 1697: 1650, 1698: 1651, 1699: 1652, 1701: 1653, 1702: 1654, 1703: 1655, 1704: 1656, 1705: 1657, 1706: 1658, 1707: 1659, 1708: 1660, 1709: 1661, 1710: 1662, 1711: 1663, 1713: 1664, 1714: 1665, 1715: 1666, 1716: 1667, 1717: 1668, 1718: 1669, 1719: 1670, 1720: 1671, 1721: 1672, 1722: 1673, 1723: 1674, 1724: 1675, 1725: 1676, 1726: 1677, 1727: 1678, 1728: 1679, 1729: 1680, 1730: 1681, 1731: 1682, 1732: 1683, 1733: 1684, 1734: 1685, 1735: 1686, 1738: 1687, 1739: 1688, 1740: 1689, 1741: 1690, 1742: 1691, 1743: 1692, 1744: 1693, 1746: 1694, 1747: 1695, 1748: 1696, 1749: 1697, 1750: 1698, 1752: 1699, 1753: 1700, 1754: 1701, 1755: 1702, 1756: 1703, 1757: 1704, 1758: 1705, 1759: 1706, 1760: 1707, 1762: 1708, 1764: 1709, 1765: 1710, 1767: 1711, 1768: 1712, 1769: 1713, 1770: 1714, 1771: 1715, 1772: 1716, 1773: 1717, 1774: 1718, 1776: 1719, 1777: 1720, 1779: 1721, 1780: 1722, 1781: 1723, 1782: 1724, 1783: 1725, 1784: 1726, 1785: 1727, 1787: 1728, 1788: 1729, 1789: 1730, 1791: 1731, 1792: 1732, 1793: 1733, 1794: 1734, 1795: 1735, 1796: 1736, 1797: 1737, 1798: 1738, 1799: 1739, 1801: 1740, 1804: 1741, 1805: 1742, 1806: 1743, 1807: 1744, 1809: 1745, 1810: 1746, 1811: 1747, 1812: 1748, 1814: 1749, 1815: 1750, 1816: 1751, 1817: 1752, 1819: 1753, 1820: 1754, 1821: 1755, 1822: 1756, 1824: 1757, 1825: 1758, 1826: 1759, 1827: 1760, 1829: 1761, 1830: 1762, 1831: 1763, 1832: 1764, 1833: 1765, 1834: 1766, 1835: 1767, 1836: 1768, 1837: 1769, 1839: 1770, 1840: 1771, 1841: 1772, 1842: 1773, 1843: 1774, 1844: 1775, 1845: 1776, 1846: 1777, 1847: 1778, 1848: 1779, 1849: 1780, 1850: 1781, 1851: 1782, 1852: 1783, 1853: 1784, 1854: 1785, 1855: 1786, 1856: 1787, 1857: 1788, 1858: 1789, 1859: 1790, 1860: 1791, 1861: 1792, 1862: 1793, 1863: 1794, 1864: 1795, 1865: 1796, 1866: 1797, 1867: 1798, 1868: 1799, 1869: 1800, 1870: 1801, 1871: 1802, 1872: 1803, 1873: 1804, 1874: 1805, 1875: 1806, 1876: 1807, 1877: 1808, 1878: 1809, 1879: 1810, 1880: 1811, 1881: 1812, 1882: 1813, 1883: 1814, 1884: 1815, 1885: 1816, 1886: 1817, 1887: 1818, 1888: 1819, 1889: 1820, 1890: 1821, 1891: 1822, 1892: 1823, 1893: 1824, 1894: 1825, 1895: 1826, 1896: 1827, 1897: 1828, 1898: 1829, 1899: 1830, 1900: 1831, 1901: 1832, 1902: 1833, 1903: 1834, 1904: 1835, 1905: 1836, 1906: 1837, 1907: 1838, 1908: 1839, 1909: 1840, 1910: 1841, 1911: 1842, 1912: 1843, 1913: 1844, 1914: 1845, 1915: 1846, 1916: 1847, 1917: 1848, 1918: 1849, 1919: 1850, 1920: 1851, 1921: 1852, 1922: 1853, 1923: 1854, 1924: 1855, 1925: 1856, 1926: 1857, 1927: 1858, 1928: 1859, 1929: 1860, 1930: 1861, 1931: 1862, 1932: 1863, 1933: 1864, 1934: 1865, 1935: 1866, 1936: 1867, 1937: 1868, 1938: 1869, 1939: 1870, 1940: 1871, 1941: 1872, 1942: 1873, 1943: 1874, 1944: 1875, 1945: 1876, 1946: 1877, 1947: 1878, 1948: 1879, 1949: 1880, 1950: 1881, 1951: 1882, 1952: 1883, 1953: 1884, 1954: 1885, 1955: 1886, 1956: 1887, 1957: 1888, 1958: 1889, 1959: 1890, 1960: 1891, 1961: 1892, 1962: 1893, 1963: 1894, 1964: 1895, 1965: 1896, 1966: 1897, 1967: 1898, 1968: 1899, 1969: 1900, 1970: 1901, 1971: 1902, 1972: 1903, 1973: 1904, 1974: 1905, 1975: 1906, 1976: 1907, 1977: 1908, 1978: 1909, 1979: 1910, 1980: 1911, 1981: 1912, 1982: 1913, 1983: 1914, 1984: 1915, 1985: 1916, 1986: 1917, 1987: 1918, 1988: 1919, 1989: 1920, 1990: 1921, 1991: 1922, 1992: 1923, 1993: 1924, 1994: 1925, 1995: 1926, 1996: 1927, 1997: 1928, 1998: 1929, 1999: 1930, 2000: 1931, 2001: 1932, 2002: 1933, 2003: 1934, 2004: 1935, 2005: 1936, 2006: 1937, 2007: 1938, 2008: 1939, 2009: 1940, 2010: 1941, 2011: 1942, 2012: 1943, 2013: 1944, 2014: 1945, 2015: 1946, 2016: 1947, 2017: 1948, 2018: 1949, 2019: 1950, 2020: 1951, 2021: 1952, 2022: 1953, 2023: 1954, 2024: 1955, 2025: 1956, 2026: 1957, 2027: 1958, 2028: 1959, 2029: 1960, 2030: 1961, 2031: 1962, 2032: 1963, 2033: 1964, 2034: 1965, 2035: 1966, 2036: 1967, 2037: 1968, 2038: 1969, 2039: 1970, 2040: 1971, 2041: 1972, 2042: 1973, 2043: 1974, 2044: 1975, 2045: 1976, 2046: 1977, 2047: 1978, 2048: 1979, 2049: 1980, 2050: 1981, 2051: 1982, 2052: 1983, 2053: 1984, 2054: 1985, 2055: 1986, 2056: 1987, 2057: 1988, 2058: 1989, 2059: 1990, 2060: 1991, 2061: 1992, 2062: 1993, 2063: 1994, 2064: 1995, 2065: 1996, 2066: 1997, 2067: 1998, 2068: 1999, 2069: 2000, 2070: 2001, 2071: 2002, 2072: 2003, 2073: 2004, 2074: 2005, 2075: 2006, 2076: 2007, 2077: 2008, 2078: 2009, 2079: 2010, 2080: 2011, 2081: 2012, 2082: 2013, 2083: 2014, 2084: 2015, 2085: 2016, 2086: 2017, 2087: 2018, 2088: 2019, 2089: 2020, 2090: 2021, 2091: 2022, 2092: 2023, 2093: 2024, 2094: 2025, 2095: 2026, 2096: 2027, 2097: 2028, 2098: 2029, 2099: 2030, 2100: 2031, 2101: 2032, 2102: 2033, 2103: 2034, 2104: 2035, 2105: 2036, 2106: 2037, 2107: 2038, 2108: 2039, 2109: 2040, 2110: 2041, 2111: 2042, 2112: 2043, 2113: 2044, 2114: 2045, 2115: 2046, 2116: 2047, 2117: 2048, 2118: 2049, 2119: 2050, 2120: 2051, 2121: 2052, 2122: 2053, 2123: 2054, 2124: 2055, 2125: 2056, 2126: 2057, 2127: 2058, 2128: 2059, 2129: 2060, 2130: 2061, 2131: 2062, 2132: 2063, 2133: 2064, 2134: 2065, 2135: 2066, 2136: 2067, 2137: 2068, 2138: 2069, 2139: 2070, 2140: 2071, 2141: 2072, 2142: 2073, 2143: 2074, 2144: 2075, 2145: 2076, 2146: 2077, 2147: 2078, 2148: 2079, 2149: 2080, 2150: 2081, 2151: 2082, 2152: 2083, 2153: 2084, 2154: 2085, 2155: 2086, 2156: 2087, 2157: 2088, 2158: 2089, 2159: 2090, 2160: 2091, 2161: 2092, 2162: 2093, 2163: 2094, 2164: 2095, 2165: 2096, 2166: 2097, 2167: 2098, 2168: 2099, 2169: 2100, 2170: 2101, 2171: 2102, 2172: 2103, 2173: 2104, 2174: 2105, 2175: 2106, 2176: 2107, 2177: 2108, 2178: 2109, 2179: 2110, 2180: 2111, 2181: 2112, 2182: 2113, 2183: 2114, 2184: 2115, 2185: 2116, 2186: 2117, 2187: 2118, 2188: 2119, 2189: 2120, 2190: 2121, 2191: 2122, 2192: 2123, 2193: 2124, 2194: 2125, 2195: 2126, 2196: 2127, 2197: 2128, 2198: 2129, 2199: 2130, 2200: 2131, 2201: 2132, 2202: 2133, 2203: 2134, 2204: 2135, 2205: 2136, 2206: 2137, 2207: 2138, 2208: 2139, 2209: 2140, 2210: 2141, 2211: 2142, 2212: 2143, 2213: 2144, 2214: 2145, 2215: 2146, 2216: 2147, 2217: 2148, 2218: 2149, 2219: 2150, 2220: 2151, 2221: 2152, 2222: 2153, 2223: 2154, 2224: 2155, 2225: 2156, 2226: 2157, 2227: 2158, 2228: 2159, 2229: 2160, 2230: 2161, 2231: 2162, 2232: 2163, 2233: 2164, 2234: 2165, 2235: 2166, 2236: 2167, 2237: 2168, 2238: 2169, 2239: 2170, 2240: 2171, 2241: 2172, 2242: 2173, 2243: 2174, 2244: 2175, 2245: 2176, 2246: 2177, 2247: 2178, 2248: 2179, 2249: 2180, 2250: 2181, 2251: 2182, 2252: 2183, 2253: 2184, 2254: 2185, 2255: 2186, 2256: 2187, 2257: 2188, 2258: 2189, 2259: 2190, 2260: 2191, 2261: 2192, 2262: 2193, 2263: 2194, 2264: 2195, 2265: 2196, 2266: 2197, 2267: 2198, 2268: 2199, 2269: 2200, 2270: 2201, 2271: 2202, 2272: 2203, 2273: 2204, 2274: 2205, 2275: 2206, 2276: 2207, 2277: 2208, 2278: 2209, 2279: 2210, 2280: 2211, 2281: 2212, 2282: 2213, 2283: 2214, 2284: 2215, 2285: 2216, 2286: 2217, 2287: 2218, 2288: 2219, 2289: 2220, 2290: 2221, 2291: 2222, 2292: 2223, 2293: 2224, 2294: 2225, 2295: 2226, 2296: 2227, 2297: 2228, 2298: 2229, 2299: 2230, 2300: 2231, 2301: 2232, 2302: 2233, 2303: 2234, 2304: 2235, 2305: 2236, 2306: 2237, 2307: 2238, 2308: 2239, 2309: 2240, 2310: 2241, 2311: 2242, 2312: 2243, 2313: 2244, 2314: 2245, 2315: 2246, 2316: 2247, 2317: 2248, 2318: 2249, 2319: 2250, 2320: 2251, 2321: 2252, 2322: 2253, 2323: 2254, 2324: 2255, 2325: 2256, 2326: 2257, 2327: 2258, 2328: 2259, 2329: 2260, 2330: 2261, 2331: 2262, 2332: 2263, 2333: 2264, 2334: 2265, 2335: 2266, 2336: 2267, 2337: 2268, 2338: 2269, 2339: 2270, 2340: 2271, 2341: 2272, 2342: 2273, 2343: 2274, 2344: 2275, 2345: 2276, 2346: 2277, 2347: 2278, 2348: 2279, 2349: 2280, 2350: 2281, 2351: 2282, 2352: 2283, 2353: 2284, 2354: 2285, 2355: 2286, 2356: 2287, 2357: 2288, 2358: 2289, 2359: 2290, 2360: 2291, 2361: 2292, 2362: 2293, 2363: 2294, 2364: 2295, 2365: 2296, 2366: 2297, 2367: 2298, 2368: 2299, 2369: 2300, 2370: 2301, 2371: 2302, 2372: 2303, 2373: 2304, 2374: 2305, 2375: 2306, 2376: 2307, 2377: 2308, 2378: 2309, 2379: 2310, 2380: 2311, 2381: 2312, 2382: 2313, 2383: 2314, 2384: 2315, 2385: 2316, 2386: 2317, 2387: 2318, 2388: 2319, 2389: 2320, 2390: 2321, 2391: 2322, 2392: 2323, 2393: 2324, 2394: 2325, 2395: 2326, 2396: 2327, 2397: 2328, 2398: 2329, 2399: 2330, 2400: 2331, 2401: 2332, 2402: 2333, 2403: 2334, 2404: 2335, 2405: 2336, 2406: 2337, 2407: 2338, 2408: 2339, 2409: 2340, 2410: 2341, 2411: 2342, 2412: 2343, 2413: 2344, 2414: 2345, 2415: 2346, 2416: 2347, 2417: 2348, 2418: 2349, 2419: 2350, 2420: 2351, 2421: 2352, 2422: 2353, 2423: 2354, 2424: 2355, 2425: 2356, 2426: 2357, 2427: 2358, 2428: 2359, 2429: 2360, 2430: 2361, 2431: 2362, 2432: 2363, 2433: 2364, 2434: 2365, 2435: 2366, 2436: 2367, 2437: 2368, 2438: 2369, 2439: 2370, 2440: 2371, 2441: 2372, 2442: 2373, 2443: 2374, 2444: 2375, 2445: 2376, 2446: 2377, 2447: 2378, 2448: 2379, 2449: 2380, 2450: 2381, 2451: 2382, 2452: 2383, 2453: 2384, 2454: 2385, 2455: 2386, 2456: 2387, 2457: 2388, 2458: 2389, 2459: 2390, 2460: 2391, 2461: 2392, 2462: 2393, 2463: 2394, 2464: 2395, 2465: 2396, 2466: 2397, 2467: 2398, 2468: 2399, 2469: 2400, 2470: 2401, 2471: 2402, 2472: 2403, 2473: 2404, 2474: 2405, 2475: 2406, 2476: 2407, 2477: 2408, 2478: 2409, 2479: 2410, 2480: 2411, 2481: 2412, 2482: 2413, 2483: 2414, 2484: 2415, 2485: 2416, 2486: 2417, 2487: 2418, 2488: 2419, 2489: 2420, 2490: 2421, 2491: 2422, 2492: 2423, 2493: 2424, 2494: 2425, 2495: 2426, 2496: 2427, 2497: 2428, 2498: 2429, 2499: 2430, 2500: 2431, 2501: 2432, 2502: 2433, 2503: 2434, 2504: 2435, 2505: 2436, 2506: 2437, 2507: 2438, 2508: 2439, 2509: 2440, 2510: 2441, 2511: 2442, 2512: 2443, 2513: 2444, 2514: 2445, 2515: 2446, 2516: 2447, 2517: 2448, 2518: 2449, 2519: 2450, 2520: 2451, 2521: 2452, 2522: 2453, 2523: 2454, 2524: 2455, 2525: 2456, 2526: 2457, 2527: 2458, 2528: 2459, 2529: 2460, 2530: 2461, 2531: 2462, 2532: 2463, 2533: 2464, 2534: 2465, 2535: 2466, 2536: 2467, 2537: 2468, 2538: 2469, 2539: 2470, 2540: 2471, 2541: 2472, 2542: 2473, 2543: 2474, 2544: 2475, 2545: 2476, 2546: 2477, 2547: 2478, 2548: 2479, 2549: 2480, 2550: 2481, 2551: 2482, 2552: 2483, 2553: 2484, 2554: 2485, 2555: 2486, 2556: 2487, 2557: 2488, 2558: 2489, 2559: 2490, 2560: 2491, 2561: 2492, 2562: 2493, 2563: 2494, 2564: 2495, 2565: 2496, 2566: 2497, 2567: 2498, 2568: 2499, 2569: 2500, 2570: 2501, 2571: 2502, 2572: 2503, 2573: 2504, 2574: 2505, 2575: 2506, 2576: 2507, 2577: 2508, 2578: 2509, 2579: 2510, 2580: 2511, 2581: 2512, 2582: 2513, 2583: 2514, 2584: 2515, 2585: 2516, 2586: 2517, 2587: 2518, 2588: 2519, 2589: 2520, 2590: 2521, 2591: 2522, 2592: 2523, 2593: 2524, 2594: 2525, 2595: 2526, 2596: 2527, 2597: 2528, 2598: 2529, 2599: 2530, 2600: 2531, 2601: 2532, 2602: 2533, 2603: 2534, 2604: 2535, 2605: 2536, 2606: 2537, 2607: 2538, 2608: 2539, 2609: 2540, 2610: 2541, 2611: 2542, 2612: 2543, 2613: 2544, 2614: 2545, 2615: 2546, 2616: 2547, 2617: 2548, 2618: 2549, 2619: 2550, 2620: 2551, 2621: 2552, 2622: 2553, 2623: 2554, 2624: 2555, 2625: 2556, 2626: 2557, 2627: 2558, 2628: 2559, 2629: 2560, 2630: 2561, 2631: 2562, 2632: 2563, 2633: 2564, 2634: 2565, 2635: 2566, 2636: 2567, 2637: 2568, 2638: 2569, 2639: 2570, 2640: 2571, 2641: 2572, 2642: 2573, 2643: 2574, 2644: 2575, 2645: 2576, 2646: 2577, 2647: 2578, 2648: 2579, 2649: 2580, 2650: 2581, 2651: 2582, 2652: 2583, 2653: 2584, 2654: 2585, 2655: 2586, 2656: 2587, 2657: 2588, 2658: 2589, 2659: 2590, 2660: 2591, 2661: 2592, 2662: 2593, 2663: 2594, 2664: 2595, 2665: 2596, 2666: 2597, 2667: 2598, 2668: 2599, 2669: 2600, 2670: 2601, 2671: 2602, 2672: 2603, 2673: 2604, 2674: 2605, 2675: 2606, 2676: 2607, 2677: 2608, 2678: 2609, 2679: 2610, 2680: 2611, 2681: 2612, 2682: 2613, 2683: 2614, 2684: 2615, 2685: 2616, 2686: 2617, 2687: 2618, 2688: 2619, 2689: 2620, 2690: 2621, 2691: 2622, 2692: 2623, 2693: 2624, 2694: 2625, 2695: 2626, 2696: 2627, 2697: 2628, 2698: 2629, 2699: 2630, 2700: 2631, 2701: 2632, 2702: 2633, 2703: 2634, 2704: 2635, 2705: 2636, 2706: 2637, 2707: 2638, 2708: 2639, 2709: 2640, 2710: 2641, 2711: 2642, 2712: 2643, 2713: 2644, 2714: 2645, 2715: 2646, 2716: 2647, 2717: 2648, 2718: 2649, 2719: 2650, 2720: 2651, 2721: 2652, 2722: 2653, 2723: 2654, 2724: 2655, 2725: 2656, 2726: 2657, 2727: 2658, 2728: 2659, 2729: 2660, 2730: 2661, 2731: 2662, 2732: 2663, 2733: 2664, 2734: 2665, 2735: 2666, 2736: 2667, 2737: 2668, 2738: 2669, 2739: 2670, 2740: 2671, 2741: 2672, 2742: 2673, 2743: 2674, 2744: 2675, 2745: 2676, 2746: 2677, 2747: 2678, 2748: 2679, 2749: 2680, 2750: 2681, 2751: 2682, 2752: 2683, 2753: 2684, 2754: 2685, 2755: 2686, 2756: 2687, 2757: 2688, 2758: 2689, 2759: 2690, 2760: 2691, 2761: 2692, 2762: 2693, 2763: 2694, 2764: 2695, 2765: 2696, 2766: 2697, 2767: 2698, 2768: 2699, 2769: 2700, 2770: 2701, 2771: 2702, 2772: 2703, 2773: 2704, 2774: 2705, 2775: 2706, 2776: 2707, 2777: 2708, 2778: 2709, 2779: 2710, 2780: 2711, 2781: 2712, 2782: 2713, 2783: 2714, 2784: 2715, 2785: 2716, 2786: 2717, 2787: 2718, 2788: 2719, 2789: 2720, 2790: 2721, 2791: 2722, 2792: 2723, 2793: 2724, 2794: 2725, 2795: 2726, 2796: 2727, 2797: 2728, 2798: 2729, 2799: 2730, 2800: 2731, 2801: 2732, 2802: 2733, 2803: 2734, 2804: 2735, 2805: 2736, 2806: 2737, 2807: 2738, 2808: 2739, 2809: 2740, 2810: 2741, 2811: 2742, 2812: 2743, 2813: 2744, 2814: 2745, 2815: 2746, 2816: 2747, 2817: 2748, 2818: 2749, 2819: 2750, 2820: 2751, 2821: 2752, 2822: 2753, 2823: 2754, 2824: 2755, 2825: 2756, 2826: 2757, 2827: 2758, 2828: 2759, 2829: 2760, 2830: 2761, 2831: 2762, 2832: 2763, 2833: 2764, 2834: 2765, 2835: 2766, 2836: 2767, 2837: 2768, 2838: 2769, 2839: 2770, 2840: 2771, 2841: 2772, 2842: 2773, 2843: 2774, 2844: 2775, 2845: 2776, 2846: 2777, 2847: 2778, 2848: 2779, 2849: 2780, 2850: 2781, 2851: 2782, 2852: 2783, 2853: 2784, 2854: 2785, 2855: 2786, 2856: 2787, 2857: 2788, 2858: 2789, 2859: 2790, 2860: 2791, 2861: 2792, 2862: 2793, 2863: 2794, 2864: 2795, 2865: 2796, 2866: 2797, 2867: 2798, 2868: 2799, 2869: 2800, 2870: 2801, 2871: 2802, 2872: 2803, 2873: 2804, 2874: 2805, 2875: 2806, 2876: 2807, 2877: 2808, 2878: 2809, 2879: 2810, 2880: 2811, 2881: 2812, 2882: 2813, 2883: 2814, 2884: 2815, 2885: 2816, 2886: 2817, 2887: 2818, 2888: 2819, 2889: 2820, 2890: 2821, 2891: 2822, 2892: 2823, 2893: 2824, 2894: 2825, 2895: 2826, 2896: 2827, 2897: 2828, 2898: 2829, 2899: 2830, 2900: 2831, 2901: 2832, 2902: 2833, 2903: 2834, 2904: 2835, 2905: 2836, 2906: 2837, 2907: 2838, 2908: 2839, 2909: 2840, 2910: 2841, 2911: 2842, 2912: 2843, 2913: 2844, 2914: 2845, 2915: 2846, 2916: 2847, 2917: 2848, 2918: 2849, 2919: 2850, 2920: 2851, 2921: 2852, 2922: 2853, 2923: 2854, 2924: 2855, 2925: 2856, 2926: 2857, 2927: 2858, 2928: 2859, 2929: 2860, 2930: 2861, 2931: 2862, 2932: 2863, 2933: 2864, 2934: 2865, 2935: 2866, 2936: 2867, 2937: 2868, 2938: 2869, 2939: 2870, 2940: 2871, 2941: 2872, 2942: 2873, 2943: 2874, 2944: 2875, 2945: 2876, 2946: 2877, 2947: 2878, 2948: 2879, 2949: 2880, 2950: 2881, 2951: 2882, 2952: 2883, 2953: 2884, 2954: 2885, 2955: 2886, 2956: 2887, 2957: 2888, 2958: 2889, 2959: 2890, 2960: 2891, 2961: 2892, 2962: 2893, 2963: 2894, 2964: 2895, 2965: 2896, 2966: 2897, 2967: 2898, 2968: 2899, 2969: 2900, 2970: 2901, 2971: 2902, 2972: 2903, 2973: 2904, 2974: 2905, 2975: 2906, 2976: 2907, 2977: 2908, 2978: 2909, 2979: 2910, 2980: 2911, 2981: 2912, 2982: 2913, 2983: 2914, 2984: 2915, 2985: 2916, 2986: 2917, 2987: 2918, 2988: 2919, 2989: 2920, 2990: 2921, 2991: 2922, 2992: 2923, 2993: 2924, 2994: 2925, 2995: 2926, 2996: 2927, 2997: 2928, 2998: 2929, 2999: 2930, 3000: 2931, 3001: 2932, 3002: 2933, 3003: 2934, 3004: 2935, 3005: 2936, 3006: 2937, 3007: 2938, 3008: 2939, 3009: 2940, 3010: 2941, 3011: 2942, 3012: 2943, 3013: 2944, 3014: 2945, 3015: 2946, 3016: 2947, 3017: 2948, 3018: 2949, 3019: 2950, 3020: 2951, 3021: 2952, 3022: 2953, 3023: 2954, 3024: 2955, 3025: 2956, 3026: 2957, 3027: 2958, 3028: 2959, 3029: 2960, 3030: 2961, 3031: 2962, 3032: 2963, 3033: 2964, 3034: 2965, 3035: 2966, 3036: 2967, 3037: 2968, 3038: 2969, 3039: 2970, 3040: 2971, 3041: 2972, 3042: 2973, 3043: 2974, 3044: 2975, 3045: 2976, 3046: 2977, 3047: 2978, 3048: 2979, 3049: 2980, 3050: 2981, 3051: 2982, 3052: 2983, 3053: 2984, 3054: 2985, 3055: 2986, 3056: 2987, 3057: 2988, 3058: 2989, 3059: 2990, 3060: 2991, 3061: 2992, 3062: 2993, 3063: 2994, 3064: 2995, 3065: 2996, 3066: 2997, 3067: 2998, 3068: 2999, 3069: 3000, 3070: 3001, 3071: 3002, 3072: 3003, 3073: 3004, 3074: 3005, 3075: 3006, 3076: 3007, 3077: 3008, 3078: 3009, 3079: 3010, 3080: 3011, 3081: 3012, 3082: 3013, 3083: 3014, 3084: 3015, 3085: 3016, 3086: 3017, 3087: 3018, 3088: 3019, 3089: 3020, 3090: 3021, 3091: 3022, 3092: 3023, 3093: 3024, 3094: 3025, 3095: 3026, 3096: 3027, 3097: 3028, 3098: 3029, 3099: 3030, 3100: 3031, 3101: 3032, 3102: 3033, 3103: 3034, 3104: 3035, 3105: 3036, 3106: 3037, 3107: 3038, 3108: 3039, 3109: 3040, 3110: 3041, 3111: 3042, 3112: 3043, 3113: 3044, 3114: 3045, 3115: 3046, 3116: 3047, 3117: 3048, 3118: 3049, 3119: 3050, 3120: 3051, 3121: 3052, 3122: 3053, 3123: 3054, 3124: 3055, 3125: 3056, 3126: 3057, 3127: 3058, 3128: 3059, 3129: 3060, 3130: 3061, 3131: 3062, 3132: 3063, 3133: 3064, 3134: 3065, 3135: 3066, 3136: 3067, 3137: 3068, 3138: 3069, 3139: 3070, 3140: 3071, 3141: 3072, 3142: 3073, 3143: 3074, 3144: 3075, 3145: 3076, 3146: 3077, 3147: 3078, 3148: 3079, 3149: 3080, 3150: 3081, 3151: 3082, 3152: 3083, 3153: 3084, 3154: 3085, 3155: 3086, 3156: 3087, 3157: 3088, 3158: 3089, 3159: 3090, 3160: 3091, 3161: 3092, 3162: 3093, 3163: 3094, 3164: 3095, 3165: 3096, 3166: 3097, 3167: 3098, 3168: 3099, 3169: 3100, 3170: 3101, 3171: 3102, 3172: 3103, 3173: 3104, 3174: 3105, 3175: 3106, 3176: 3107, 3177: 3108, 3178: 3109, 3179: 3110, 3180: 3111, 3181: 3112, 3182: 3113, 3183: 3114, 3184: 3115, 3185: 3116, 3186: 3117, 3187: 3118, 3188: 3119, 3189: 3120, 3190: 3121, 3191: 3122, 3192: 3123, 3193: 3124, 3194: 3125, 3195: 3126, 3196: 3127, 3197: 3128, 3198: 3129, 3199: 3130, 3200: 3131, 3201: 3132, 3202: 3133, 3203: 3134, 3204: 3135, 3205: 3136, 3206: 3137, 3207: 3138, 3208: 3139, 3209: 3140, 3210: 3141, 3211: 3142, 3212: 3143, 3213: 3144, 3214: 3145, 3215: 3146, 3216: 3147, 3217: 3148, 3218: 3149, 3219: 3150, 3220: 3151, 3221: 3152, 3222: 3153, 3223: 3154, 3224: 3155, 3225: 3156, 3226: 3157, 3227: 3158, 3228: 3159, 3229: 3160, 3230: 3161, 3231: 3162, 3232: 3163, 3233: 3164, 3234: 3165, 3235: 3166, 3236: 3167, 3237: 3168, 3238: 3169, 3239: 3170, 3240: 3171, 3241: 3172, 3242: 3173, 3243: 3174, 3244: 3175, 3245: 3176, 3246: 3177, 3247: 3178, 3248: 3179, 3249: 3180, 3250: 3181, 3251: 3182, 3252: 3183, 3253: 3184, 3254: 3185, 3255: 3186, 3256: 3187, 3257: 3188, 3258: 3189, 3259: 3190, 3260: 3191, 3261: 3192, 3262: 3193, 3263: 3194, 3264: 3195, 3265: 3196, 3266: 3197, 3267: 3198, 3268: 3199, 3269: 3200, 3270: 3201, 3271: 3202, 3272: 3203, 3273: 3204, 3274: 3205, 3275: 3206, 3276: 3207, 3277: 3208, 3278: 3209, 3279: 3210, 3280: 3211, 3281: 3212, 3282: 3213, 3283: 3214, 3284: 3215, 3285: 3216, 3286: 3217, 3287: 3218, 3288: 3219, 3289: 3220, 3290: 3221, 3291: 3222, 3292: 3223, 3293: 3224, 3294: 3225, 3295: 3226, 3296: 3227, 3297: 3228, 3298: 3229, 3299: 3230, 3300: 3231, 3301: 3232, 3302: 3233, 3303: 3234, 3304: 3235, 3305: 3236, 3306: 3237, 3307: 3238, 3308: 3239, 3309: 3240, 3310: 3241, 3311: 3242, 3312: 3243, 3313: 3244, 3314: 3245, 3315: 3246, 3316: 3247, 3317: 3248, 3318: 3249, 3319: 3250, 3320: 3251, 3321: 3252, 3322: 3253, 3323: 3254, 3324: 3255, 3325: 3256, 3326: 3257, 3327: 3258, 3328: 3259, 3329: 3260, 3330: 3261, 3331: 3262, 3332: 3263, 3333: 3264, 3334: 3265, 3335: 3266, 3336: 3267, 3337: 3268, 3338: 3269, 3339: 3270, 3340: 3271, 3341: 3272, 3342: 3273, 3343: 3274, 3344: 3275, 3345: 3276, 3346: 3277, 3347: 3278, 3348: 3279, 3349: 3280, 3350: 3281, 3351: 3282, 3352: 3283, 3353: 3284, 3354: 3285, 3355: 3286, 3356: 3287, 3357: 3288, 3358: 3289, 3359: 3290, 3360: 3291, 3361: 3292, 3362: 3293, 3363: 3294, 3364: 3295, 3365: 3296, 3366: 3297, 3367: 3298, 3368: 3299, 3369: 3300, 3370: 3301, 3371: 3302, 3372: 3303, 3373: 3304, 3374: 3305, 3375: 3306, 3376: 3307, 3377: 3308, 3378: 3309, 3379: 3310, 3380: 3311, 3381: 3312, 3382: 3313, 3383: 3314, 3384: 3315, 3385: 3316, 3386: 3317, 3387: 3318, 3388: 3319, 3389: 3320, 3390: 3321, 3391: 3322, 3392: 3323, 3393: 3324, 3394: 3325, 3395: 3326, 3396: 3327, 3397: 3328, 3398: 3329, 3399: 3330, 3400: 3331, 3401: 3332, 3402: 3333, 3403: 3334, 3404: 3335, 3405: 3336, 3406: 3337, 3407: 3338, 3408: 3339, 3409: 3340, 3410: 3341, 3411: 3342, 3412: 3343, 3413: 3344, 3414: 3345, 3415: 3346, 3416: 3347, 3417: 3348, 3418: 3349, 3419: 3350, 3420: 3351, 3421: 3352, 3422: 3353, 3423: 3354, 3424: 3355, 3425: 3356, 3426: 3357, 3427: 3358, 3428: 3359, 3429: 3360, 3430: 3361, 3431: 3362, 3432: 3363, 3433: 3364, 3434: 3365, 3435: 3366, 3436: 3367, 3437: 3368, 3438: 3369, 3439: 3370, 3440: 3371, 3441: 3372, 3442: 3373, 3443: 3374, 3444: 3375, 3445: 3376, 3446: 3377, 3447: 3378, 3448: 3379, 3449: 3380, 3450: 3381, 3451: 3382, 3452: 3383, 3453: 3384, 3454: 3385, 3455: 3386, 3456: 3387, 3457: 3388, 3458: 3389, 3459: 3390, 3460: 3391, 3461: 3392, 3462: 3393, 3463: 3394, 3464: 3395, 3465: 3396, 3466: 3397, 3467: 3398, 3468: 3399, 3469: 3400, 3470: 3401, 3471: 3402, 3472: 3403, 3473: 3404, 3474: 3405, 3475: 3406, 3476: 3407, 3477: 3408, 3478: 3409, 3479: 3410, 3480: 3411, 3481: 3412, 3482: 3413, 3483: 3414, 3484: 3415, 3485: 3416, 3486: 3417, 3487: 3418, 3488: 3419, 3489: 3420, 3490: 3421, 3491: 3422, 3492: 3423, 3493: 3424, 3494: 3425, 3495: 3426, 3496: 3427, 3497: 3428, 3498: 3429, 3499: 3430, 3500: 3431, 3501: 3432, 3502: 3433, 3503: 3434, 3504: 3435, 3505: 3436, 3506: 3437, 3507: 3438, 3508: 3439, 3509: 3440, 3510: 3441, 3511: 3442, 3512: 3443, 3513: 3444, 3514: 3445, 3515: 3446, 3516: 3447, 3517: 3448, 3518: 3449, 3519: 3450, 3520: 3451, 3521: 3452, 3522: 3453, 3523: 3454, 3524: 3455, 3525: 3456, 3526: 3457, 3527: 3458, 3528: 3459, 3529: 3460, 3530: 3461, 3531: 3462, 3532: 3463, 3533: 3464, 3534: 3465, 3535: 3466, 3536: 3467, 3537: 3468, 3538: 3469, 3539: 3470, 3540: 3471, 3541: 3472, 3542: 3473, 3543: 3474, 3544: 3475, 3545: 3476, 3546: 3477, 3547: 3478, 3548: 3479, 3549: 3480, 3550: 3481, 3551: 3482, 3552: 3483, 3553: 3484, 3554: 3485, 3555: 3486, 3556: 3487, 3557: 3488, 3558: 3489, 3559: 3490, 3560: 3491, 3561: 3492, 3562: 3493, 3563: 3494, 3564: 3495, 3565: 3496, 3566: 3497, 3567: 3498, 3568: 3499, 3569: 3500, 3570: 3501, 3571: 3502, 3572: 3503, 3573: 3504, 3574: 3505, 3575: 3506, 3576: 3507, 3577: 3508, 3578: 3509, 3579: 3510, 3580: 3511, 3581: 3512, 3582: 3513, 3583: 3514, 3584: 3515, 3585: 3516, 3586: 3517, 3587: 3518, 3588: 3519, 3589: 3520, 3590: 3521, 3591: 3522, 3592: 3523, 3593: 3524, 3594: 3525, 3595: 3526, 3596: 3527, 3597: 3528, 3598: 3529, 3599: 3530, 3600: 3531, 3601: 3532, 3602: 3533, 3603: 3534, 3604: 3535, 3605: 3536, 3606: 3537, 3607: 3538, 3608: 3539, 3609: 3540, 3610: 3541, 3611: 3542, 3612: 3543, 3613: 3544, 3614: 3545, 3615: 3546, 3616: 3547, 3617: 3548, 3618: 3549, 3619: 3550, 3620: 3551, 3621: 3552, 3622: 3553, 3623: 3554, 3624: 3555, 3625: 3556, 3626: 3557, 3627: 3558, 3628: 3559, 3629: 3560, 3630: 3561, 3631: 3562, 3632: 3563, 3633: 3564, 3634: 3565, 3635: 3566, 3636: 3567, 3637: 3568, 3638: 3569, 3639: 3570, 3640: 3571, 3641: 3572, 3642: 3573, 3643: 3574, 3644: 3575, 3645: 3576, 3646: 3577, 3647: 3578, 3648: 3579, 3649: 3580, 3650: 3581, 3651: 3582, 3652: 3583, 3653: 3584, 3654: 3585, 3655: 3586, 3656: 3587, 3657: 3588, 3658: 3589, 3659: 3590, 3660: 3591, 3661: 3592, 3662: 3593, 3663: 3594, 3664: 3595, 3665: 3596, 3666: 3597, 3667: 3598, 3668: 3599, 3669: 3600, 3670: 3601, 3671: 3602, 3672: 3603, 3673: 3604, 3674: 3605, 3675: 3606, 3676: 3607, 3677: 3608, 3678: 3609, 3679: 3610, 3680: 3611, 3681: 3612, 3682: 3613, 3683: 3614, 3684: 3615, 3685: 3616, 3686: 3617, 3687: 3618, 3688: 3619, 3689: 3620, 3690: 3621, 3691: 3622, 3692: 3623, 3693: 3624, 3694: 3625, 3695: 3626, 3696: 3627, 3697: 3628, 3698: 3629, 3699: 3630, 3700: 3631, 3701: 3632, 3702: 3633, 3703: 3634, 3704: 3635, 3705: 3636, 3706: 3637, 3707: 3638, 3708: 3639, 3709: 3640, 3710: 3641, 3711: 3642, 3712: 3643, 3713: 3644, 3714: 3645, 3715: 3646, 3716: 3647, 3717: 3648, 3718: 3649, 3719: 3650, 3720: 3651, 3721: 3652, 3722: 3653, 3723: 3654, 3724: 3655, 3725: 3656, 3726: 3657, 3727: 3658, 3728: 3659, 3729: 3660, 3730: 3661, 3731: 3662, 3732: 3663, 3733: 3664, 3734: 3665, 3735: 3666, 3736: 3667, 3737: 3668, 3738: 3669, 3739: 3670, 3740: 3671, 3741: 3672, 3742: 3673, 3743: 3674, 3744: 3675, 3745: 3676, 3746: 3677, 3747: 3678, 3748: 3679, 3749: 3680, 3750: 3681, 3751: 3682, 3752: 3683, 3753: 3684, 3754: 3685, 3755: 3686, 3756: 3687, 3757: 3688, 3758: 3689, 3759: 3690, 3760: 3691, 3761: 3692, 3762: 3693, 3763: 3694, 3764: 3695, 3765: 3696, 3766: 3697, 3767: 3698, 3768: 3699, 3769: 3700, 3770: 3701, 3771: 3702, 3772: 3703, 3773: 3704, 3774: 3705, 3775: 3706, 3776: 3707, 3777: 3708, 3778: 3709, 3779: 3710, 3780: 3711, 3781: 3712, 3782: 3713, 3783: 3714, 3784: 3715, 3785: 3716, 3786: 3717, 3787: 3718, 3788: 3719, 3789: 3720, 3790: 3721, 3791: 3722, 3792: 3723, 3793: 3724, 3794: 3725, 3795: 3726, 3796: 3727, 3797: 3728, 3798: 3729, 3799: 3730, 3800: 3731, 3801: 3732, 3802: 3733, 3803: 3734, 3804: 3735, 3805: 3736, 3806: 3737, 3807: 3738, 3808: 3739, 3809: 3740, 3810: 3741, 3811: 3742, 3812: 3743, 3813: 3744, 3814: 3745, 3816: 3746, 3817: 3747, 3818: 3748, 3819: 3749, 3820: 3750, 3821: 3751, 3822: 3752, 3823: 3753, 3824: 3754, 3825: 3755, 3826: 3756, 3827: 3757, 3828: 3758, 3829: 3759, 3830: 3760, 3831: 3761, 3832: 3762, 3833: 3763, 3834: 3764, 3835: 3765, 3836: 3766, 3837: 3767, 3838: 3768, 3839: 3769, 3840: 3770, 3841: 3771, 3842: 3772, 3843: 3773, 3844: 3774, 3845: 3775, 3846: 3776, 3847: 3777, 3848: 3778, 3849: 3779, 3850: 3780, 3851: 3781, 3852: 3782, 3853: 3783, 3854: 3784, 3855: 3785, 3856: 3786, 3857: 3787, 3858: 3788, 3859: 3789, 3860: 3790, 3861: 3791, 3862: 3792, 3863: 3793, 3864: 3794, 3865: 3795, 3866: 3796, 3867: 3797, 3868: 3798, 3869: 3799, 3870: 3800, 3871: 3801, 3872: 3802, 3873: 3803, 3874: 3804, 3875: 3805, 3876: 3806, 3877: 3807, 3878: 3808, 3879: 3809, 3880: 3810, 3881: 3811, 3882: 3812, 3883: 3813, 3884: 3814, 3885: 3815, 3886: 3816, 3887: 3817, 3888: 3818, 3889: 3819, 3890: 3820, 3891: 3821, 3892: 3822, 3893: 3823, 3894: 3824, 3895: 3825, 3896: 3826, 3897: 3827, 3898: 3828, 3899: 3829, 3900: 3830, 3901: 3831, 3902: 3832, 3903: 3833, 3904: 3834, 3905: 3835, 3906: 3836, 3907: 3837, 3908: 3838, 3909: 3839, 3910: 3840, 3911: 3841, 3912: 3842, 3913: 3843, 3914: 3844, 3915: 3845, 3916: 3846, 3917: 3847, 3918: 3848, 3919: 3849, 3920: 3850, 3921: 3851, 3922: 3852, 3923: 3853, 3924: 3854, 3925: 3855, 3926: 3856, 3927: 3857, 3928: 3858, 3929: 3859, 3930: 3860, 3931: 3861, 3932: 3862, 3933: 3863, 3934: 3864, 3935: 3865, 3936: 3866, 3937: 3867, 3938: 3868, 3939: 3869, 3940: 3870, 3941: 3871, 3942: 3872, 3943: 3873, 3944: 3874, 3945: 3875, 3946: 3876, 3947: 3877, 3948: 3878, 3949: 3879, 3950: 3880, 3951: 3881, 3952: 3882}\n"
     ]
    }
   ],
   "source": [
    "print(nmovieIdMapIdx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
